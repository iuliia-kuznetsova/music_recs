{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a4ab1c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d04e7-1b0b-4e4a-9ddf-4526d7fef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import joblib\n",
    "import gc\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf14d14",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d99d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "datasets = {\n",
    "    'tracks.parquet': os.getenv('RAW_URL_TRACKS'),\n",
    "    'catalog_names.parquet': os.getenv('RAW_URL_CATALOG_NAMES'),\n",
    "    'interactions.parquet': os.getenv('RAW_URL_INTERACTIONS'),\n",
    "}\n",
    "\n",
    "raw_dir = os.getenv('RAW_DATA_DIR', '../data/raw')\n",
    "preprocessed_dir = os.getenv('PREPROCESSED_DATA_DIR', '../data/preprocessed')\n",
    "encoder_dir = os.getenv('ENCODERS_DIR', '../encoders')\n",
    "\n",
    "s3_bucket = os.getenv('S3_BUCKET_NAME')\n",
    "s3_prefix = os.getenv('S3_PREFIX', 'recsys/data/')\n",
    "s3_region = os.getenv('S3_REGION', 'us-east-1')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80fbc5-b660-4fac-8fbb-a5cae77313b3",
   "metadata": {},
   "source": [
    "## Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load datasets ---------- #\n",
    "items = pl.read_parquet(os.path.join(preprocessed_dir, 'items.parquet'))\n",
    "events = pl.read_parquet(os.path.join(preprocessed_dir, 'events.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e73960-fd38-4e15-8db0-9a25c35dfd25",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Check data summary ---------- #\n",
    "\n",
    "def data_summary(df: pd.DataFrame, name: str):\n",
    "    '''\n",
    "        Display a quick overview of a DataFrame.\n",
    "    '''\n",
    "\n",
    "    print(f'\\n===== {name.upper()} =====')  \n",
    "  \n",
    "    # Sample rows\n",
    "    print('\\nSample rows:')\n",
    "    display(df.head())\n",
    "\n",
    "    # Shape\n",
    "    rows, cols = df.shape\n",
    "    print(f'\\nShape: {rows} rows x {cols} columns')\n",
    "    \n",
    "    # Data info\n",
    "    print('\\nSummary for numeric columns:')\n",
    "    print(df.describe())\n",
    "   \n",
    "    # Unique values (column-wise, skip if error occurs)\n",
    "    print('\\nUnique values (for each column):')\n",
    "    try:\n",
    "        for col in df.columns:\n",
    "            print(f'\\nColumn: {col}')\n",
    "            print(df[col].value_counts())\n",
    "    except Exception as e:\n",
    "        print(f'Skipped value_counts due to error: {e}')\n",
    "    \n",
    "    # Missing values\n",
    "    print('\\nMissing values:')\n",
    "    print(df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Items data summary ---------- #\n",
    "\n",
    "#data_summary(items, 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Events data summary ---------- #\n",
    "\n",
    "data_summary(events, 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_catalog_clean = pl.read_parquet(os.path.join(preprocessed_dir, 'tracks_catalog_clean.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tracks_catalog_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- Top tracks by popularity ---------- #\n",
    "\n",
    "top_tracks_by_listen_number = (\n",
    "    events\n",
    "        .group_by('track_id')\n",
    "        .agg(pl.sum('listen_count').alias('total_listen_count'))\n",
    "        .join(tracks_catalog_clean.select(['track_id', 'track_clean']), on='track_id', how='left')\n",
    "        .sort('total_listen_count', descending=True)\n",
    "        .head(10)\n",
    ")\n",
    "\n",
    "display(top_tracks_by_listen_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Most popular track versions (current behavior)\n",
    "print(\"=== Top Track Versions ===\")\n",
    "top_track_versions = (\n",
    "    events\n",
    "    .group_by('track_id')\n",
    "    .agg(pl.sum('listen_count').alias('total_listens'))\n",
    "    .join(tracks_catalog_clean.select(['track_id', 'track_clean', 'track_group_id']), \n",
    "          on='track_id', how='left')\n",
    "    .sort('total_listens', descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "display(top_track_versions)\n",
    "\n",
    "# 2. Most popular songs (versions aggregated)\n",
    "print(\"\\n=== Top Songs (All Versions Combined) ===\")\n",
    "top_songs = (\n",
    "    events\n",
    "    .join(tracks_catalog_clean.select(['track_id', 'track_clean', 'track_group_id']), \n",
    "          on='track_id', how='left')\n",
    "    .group_by(['track_group_id', 'track_clean'])\n",
    "    .agg([\n",
    "        pl.sum('listen_count').alias('total_listens'),\n",
    "        pl.n_unique('track_id').alias('num_versions'),\n",
    "    ])\n",
    "    .sort('total_listens', descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "display(top_songs)\n",
    "\n",
    "# 3. Analysis: How many top tracks are just different versions?\n",
    "print(\"\\n=== Diversity Analysis ===\")\n",
    "unique_groups_in_top_100 = (\n",
    "    top_track_versions.head(100)['track_group_id'].n_unique()\n",
    ")\n",
    "print(f\"Top 100 tracks represent {unique_groups_in_top_100} unique songs\")\n",
    "print(f\"Duplication rate: {(100 - unique_groups_in_top_100) / 100:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c32a5a-d3be-4f96-8dd9-f7860951020c",
   "metadata": {},
   "source": [
    "Наиболее популярные жанры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 genres by listening number\n",
    "genres_by_listen_count = (\n",
    "    interactions\n",
    "        .group_by('track_id')\n",
    "        .agg(pl.len().alias('track_listen_count'))\n",
    "        .join(\n",
    "            items.select(['track_id', 'genre_clean']).unique(['track_id', 'genre_clean']),\n",
    "            on='track_id',\n",
    "            how='left'\n",
    "        )\n",
    "        .group_by('genre_clean')\n",
    "        .agg(pl.sum('track_listen_count').alias('listen_count'))\n",
    "        .sort('listen_count', descending=True)\n",
    ")\n",
    "\n",
    "top_5_genres = genres_by_listen_count.head(5)\n",
    "display(top_5_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15254e66-e80e-473b-ba24-abebea5ccac7",
   "metadata": {},
   "source": [
    "Треки, которые никто не прослушал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54020fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks that haven't been listened to by anybody\n",
    "unlistened_tracks = (\n",
    "    items\n",
    "        .select(['track_id', 'track_clean', 'artist_clean', 'album_clean', 'genre_clean'])\n",
    "        .unique('track_id')\n",
    "        .join(\n",
    "            interactions.select('track_id').unique(),\n",
    "            on='track_id',\n",
    "            how='anti'\n",
    "        )\n",
    ")\n",
    "\n",
    "print(f'Number of unlistened tracks: {unlistened_tracks.height}')\n",
    "print(unlistened_tracks.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d850a07-ef1e-462f-891a-1cf89f2e24ef",
   "metadata": {},
   "source": [
    "# Преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcf8d2-1192-4df5-b20b-fbb84689f57a",
   "metadata": {},
   "source": [
    "Преобразуем данные в формат, более пригодный для дальнейшего использования в расчётах рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up events dataset by aggregating interactions: count listens per user-track pair\n",
    "# Polars lazy mode with streaming used in order not to crash kernel by dealing with the whole dataset at once\n",
    "events = (\n",
    "    interactions.lazy()\n",
    "        .group_by(['user_id', 'track_id'])\n",
    "        .agg([\n",
    "            pl.len().alias('listen_count'),\n",
    "            pl.max('started_at').alias('last_listen')\n",
    "        ])\n",
    "        .sort(['user_id', 'listen_count'], descending=[False, True])\n",
    "        .collect(engine='streaming')\n",
    ")\n",
    "print(f\"Aggregated interactions: {events.shape}\")\n",
    "print(events.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68241760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders for user_id and track_id\n",
    "# ALS requires consecutive integer indices starting from 0\n",
    "user_encoder = LabelEncoder()\n",
    "track_encoder = LabelEncoder()\n",
    "# Fit encoders\n",
    "user_encoder.fit(events['user_id'].to_numpy())\n",
    "track_encoder.fit(events['track_id'].to_numpy())\n",
    "# Transform to encoded indices\n",
    "events_data = (\n",
    "    events\n",
    "        .with_columns([\n",
    "            pl.Series('user_idx', user_encoder.transform(events['user_id'].to_numpy())),\n",
    "            pl.Series('track_idx', track_encoder.transform(events['track_id'].to_numpy()))\n",
    "        ])\n",
    ")\n",
    "print(f\"Encoded data shape: {events_data.shape}\")\n",
    "print(f\"Unique users: {events_data['user_idx'].n_unique()}\")\n",
    "print(f\"Unique tracks: {events_data['track_idx'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scipy sparse matrix (COO format)\n",
    "# Using log-transformed listen counts as implicit feedback weights\n",
    "user_track_sparse = scipy.sparse.coo_matrix(\n",
    "    (\n",
    "        np.log1p(events_data['listen_count'].to_numpy()),  # values (log-scaled)\n",
    "        (\n",
    "            events_data['user_idx'].to_numpy(),  # row indices\n",
    "            events_data['track_idx'].to_numpy()   # col indices\n",
    "        )\n",
    "    ),\n",
    "    shape=(\n",
    "        events_data['user_idx'].max() + 1,\n",
    "        events_data['track_idx'].max() + 1\n",
    "    )\n",
    ")\n",
    "# Convert to CSR format for efficient row operations\n",
    "user_track_sparse = user_track_sparse.tocsr()\n",
    "print(f\"Sparse matrix shape: {user_track_sparse.shape}\")\n",
    "print(f\"Sparsity: {1 - user_track_sparse.nnz / (user_track_sparse.shape[0] * user_track_sparse.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6272bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoders for decoding predictions back to original ids\n",
    "encoder_mappings = {\n",
    "    'user_encoder': user_encoder,\n",
    "    'track_encoder': track_encoder,\n",
    "    'user_id_to_idx': dict(zip(events['user_id'].unique(), \n",
    "                                user_encoder.transform(events['user_id'].unique()))),\n",
    "    'track_id_to_idx': dict(zip(events['track_id'].unique(),\n",
    "                                 track_encoder.transform(events['track_id'].unique())))\n",
    "}\n",
    "\n",
    "# Save encoder locally\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "with open(os.path.join(encoder_dir, 'encoder_mappings.joblib'), 'wb') as f:\n",
    "    joblib.dump(encoder_mappings, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd8a7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1be653-eed8-4fa0-a9c5-3811b080d71d",
   "metadata": {},
   "source": [
    "# Сохранение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1314a-3af3-4faf-a0b1-3045173d1500",
   "metadata": {},
   "source": [
    "Сохраним данные в двух файлах в персональном S3-бакете по пути `recsys/data/`:\n",
    "- `items.parquet` — все данные о музыкальных треках,\n",
    "- `events.parquet` — все данные о взаимодействиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77af17-7705-4fae-9aad-db579b39faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets locally\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "items.write_parquet(os.path.join(preprocessed_dir, 'items.parquet'))\n",
    "events.write_parquet(os.path.join(preprocessed_dir, 'events.parquet'))\n",
    "print(f'Files saved locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48098d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save data to S3 bucket\n",
    "#\n",
    "## Initialize S3 client\n",
    "#s3_client = boto3.client(\n",
    "#    's3',\n",
    "#    region_name=s3_region,\n",
    "#    aws_access_key_id=aws_access_key_id,\n",
    "#    aws_secret_access_key=aws_secret_access_key\n",
    "#)\n",
    "#\n",
    "#def upload_to_s3(local_path, s3_key):\n",
    "#    '''\n",
    "#        Upload a file to S3 bucket\n",
    "#    '''\n",
    "#    try:\n",
    "#        s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "#        print(f'Uploaded {local_path} to s3://{s3_bucket}/{s3_key}')\n",
    "#    except ClientError as e:\n",
    "#        print(f'Error uploading {local_path}: {e}')\n",
    "#        raise\n",
    "#\n",
    "## Upload to S3\n",
    "#upload_to_s3(\n",
    "#    os.path.join(preprocessed_dir, 'items.parquet'),\n",
    "#    f'{s3_prefix}items.parquet'\n",
    "#)\n",
    "#upload_to_s3(\n",
    "#    os.path.join(preprocessed_dir, 'events.parquet'),\n",
    "#    f'{s3_prefix}events.parquet'\n",
    "#)\n",
    "#\n",
    "#print(f'All files uploaded to S3 bucket: {s3_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecbbed-c560-44d9-9c14-86c7dc76f399",
   "metadata": {},
   "source": [
    "# Очистка памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5358ede-ba6e-4c4f-bd73-5b9344f0ba79",
   "metadata": {},
   "source": [
    "Здесь, может понадобится очистка памяти для высвобождения ресурсов для выполнения кода ниже. \n",
    "\n",
    "Приведите соответствующие код, комментарии, например:\n",
    "- код для удаление более ненужных переменных,\n",
    "- комментарий, что следует перезапустить kernel, выполнить такие-то начальные секции и продолжить с этапа 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fe920-e12e-4ad8-b04e-56e8091fac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unnecessary variables to free memory\n",
    "\n",
    "# List of objects to delete\n",
    "variables_to_delete = [\n",
    "    'interactions',\n",
    "    'tracks',\n",
    "    'catalog_names',\n",
    "    'tracks_exploded',\n",
    "    'user_track_interactions',\n",
    "    'albums_catalog',\n",
    "    'artists_catalog',\n",
    "    'genres_catalog',\n",
    "    'tracks_catalog',\n",
    "    'album_duplicates',\n",
    "    'artist_duplicates',\n",
    "    'genre_duplicates',\n",
    "    'track_duplicates',\n",
    "    'album_id_map',\n",
    "    'artist_id_map',\n",
    "    'genre_id_map',\n",
    "    'track_id_map',\n",
    "    'albums_dedup',\n",
    "    'artists_dedup',\n",
    "    'genres_dedup',\n",
    "    'tracks_dedup'\n",
    "]\n",
    "\n",
    "# Delete variables\n",
    "for var in variables_to_delete:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "        print(f\"Deleted {var}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print('Memory cleanup complete')\n",
    "print('To fully free memory, restart the kernel:')\n",
    "print('  1. Click \"Kernel\" → \"Restart Kernel...\"')\n",
    "print('  2. Re-run initial cells:')\n",
    "print('     - Cell 3: Imports')\n",
    "print('     - Cell 5: Config')\n",
    "print('  3. Load preprocessed data:')\n",
    "print(\"     items = pl.read_parquet('../data/preprocessed/items.parquet')\")\n",
    "print(\"     events = pl.read_parquet('../data/preprocessed/events.parquet')\")\n",
    "print(\"  4. Continue from Stage 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708503df-ee89-4cf3-8489-093dc478e2a8",
   "metadata": {},
   "source": [
    "# === ЭТАП 3 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77de22-e10f-4b42-85c1-8fb6f805fe68",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a4241-1ecd-4a3e-bbb3-fc2f6ca94f68",
   "metadata": {},
   "source": [
    "Если необходимо, то загружаем items.parquet, events.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fc8a5-bd2c-40d7-864a-ee75aca6d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#items = pl.read_parquet('../data/preprocessed/items.parquet')\n",
    "#events = pl.read_parquet('../data/preprocessed/events.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694c023-6477-490b-939d-1cfa6f5f1b72",
   "metadata": {},
   "source": [
    "# Разбиение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5f6e0-54e7-4428-8678-eabce505d82c",
   "metadata": {},
   "source": [
    "Разбиваем данные на тренировочную, тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2dfa5-d8a2-47d1-922e-6eefee2c62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Split data chronologically ---------- #\n",
    "\n",
    "# Define split date\n",
    "# Convert date to days since epoch, find quantile, convert back as Polars cannot handle date quantiles\n",
    "date_threshold = (\n",
    "    events\n",
    "        .select(\n",
    "            pl.col('last_listen')\n",
    "              .cast(pl.Date)\n",
    "              .to_physical()\n",
    "              .quantile(0.8)\n",
    "              .cast(pl.Int32)\n",
    "        )\n",
    "        .item()\n",
    ")\n",
    "\n",
    "# Convert back to date\n",
    "date_threshold = pl.Series([date_threshold]).cast(pl.Date).item()\n",
    "print(f'Split date: {date_threshold}')\n",
    "\n",
    "# Split based on time\n",
    "train_events = events.filter(pl.col('last_listen') <= date_threshold)\n",
    "test_events = events.filter(pl.col('last_listen') > date_threshold)\n",
    "\n",
    "print(f'Train set: {train_events.shape[0]:,}')\n",
    "print(f'Test set: {test_events.shape[0]:,}')\n",
    "print(f'Split ratio: {train_events.shape[0]/events.shape[0]:.1%} / {test_events.shape[0]/events.shape[0]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131c7e6-8852-4556-b510-51f7253cc299",
   "metadata": {},
   "source": [
    "# Топ популярных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70d43a-88cc-4719-b291-feaed7136f30",
   "metadata": {},
   "source": [
    "Рассчитаем рекомендации как топ популярных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45e200-b7d6-4f56-9077-aad431689b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find popularity score from training data and get top 100 tracks\n",
    "\n",
    "def get_popular_tracks(train_events, tracks_catalog, items=None, top_n=100, min_users=10, max_avg_listens=50):\n",
    "    ''' \n",
    "        Get most popular tracks with protection against data corruption.\n",
    "        \n",
    "        Parameters:\n",
    "        - train_events: aggregated user-track interactions\n",
    "        - tracks_catalog: deduplicated track catalog (tracks_catalog_clean)\n",
    "        - items: optional DataFrame for genre/artist info\n",
    "        - top_n: number of top tracks to return\n",
    "        - min_users: minimum unique users required\n",
    "        - max_avg_listens: maximum average listens per user (anti-bot protection)\n",
    "        \n",
    "        Anti-bot protection applied as filters of:\n",
    "        - minimum user_count,\n",
    "        - maximum average listens per user.\n",
    "        \n",
    "        Popularity score is calculated as multiplicative combination of \n",
    "        log(total_listens) * log(user_count), \n",
    "        so low user_count works as penalty and drastically reduces score.\n",
    "\n",
    "        Return is the top-N tracks with highest popularity score, each track_id \n",
    "        combined with the most common genre and artists\n",
    "        so that track in the top is original and not a remix or cover.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with track_id, popularity_score, and optional genre, artist, album. \n",
    "    '''\n",
    "    \n",
    "    popular_tracks = (\n",
    "        train_events\n",
    "            .group_by('track_id')\n",
    "            .agg([\n",
    "                pl.sum('listen_count').alias('total_listens'),\n",
    "                pl.len().alias('user_count')  \n",
    "            ])\n",
    "            # Calculate average listens per user\n",
    "            .with_columns([\n",
    "                (pl.col('total_listens') / pl.col('user_count')).alias('avg_per_user')\n",
    "            ])\n",
    "            # Filter suspicious tracks\n",
    "            .filter(\n",
    "                (pl.col('user_count') >= min_users) &  # Minimum user diversity\n",
    "                (pl.col('avg_per_user') <= max_avg_listens)  # Anti-bot filter\n",
    "            )\n",
    "            # Multiplicative popularity score (both must be high in order to get a high popularity score)\n",
    "            .with_columns([\n",
    "                (pl.col('total_listens').log1p() * \n",
    "                 pl.col('user_count').log1p()).alias('popularity_score')\n",
    "            ])\n",
    "            .sort('popularity_score', descending=True)\n",
    "            .head(top_n)\n",
    "    )\n",
    "    \n",
    "    # Join with deduplicated track catalog to get unique track names\n",
    "    popular_tracks_with_info = (\n",
    "        popular_tracks\n",
    "            .join(\n",
    "                tracks_catalog.select(['track_id', 'track_clean']),\n",
    "                on='track_id',\n",
    "                how='left'\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    # Add most common genre and artist for each track\n",
    "    if items is not None:\n",
    "        track_meta = (\n",
    "            items\n",
    "                .group_by('track_id')\n",
    "                .agg([\n",
    "                    pl.col('genre_clean').mode().first().alias('genre_clean'),\n",
    "                    pl.col('artist_clean').mode().first().alias('artist_clean'),\n",
    "                    pl.col('album_clean').mode().first().alias('album_clean')\n",
    "                ])\n",
    "        )\n",
    "        popular_tracks_with_info = popular_tracks_with_info.join(\n",
    "            track_meta,\n",
    "            on='track_id',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return popular_tracks_with_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 100 popular tracks\n",
    "top_popular = get_popular_tracks(\n",
    "    train_events=train_events, \n",
    "    tracks_catalog=tracks_catalog_clean,\n",
    "    items=items,\n",
    "    top_n=100, \n",
    "    min_users=10, \n",
    "    max_avg_listens=50\n",
    ")\n",
    "\n",
    "print(f'Top 10 Popular Tracks:')\n",
    "display(top_popular.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad231f2-6158-421a-b7fa-01d8bc3ed572",
   "metadata": {},
   "source": [
    "# Персональные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86159460-cd9d-4b63-8248-604ea3c9aebf",
   "metadata": {},
   "source": [
    "Рассчитаем персональные рекомендации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cdb58-3a8c-45ad-8e5f-7f950314aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Prepare Data for ALS Model\n",
    "\n",
    "# Encode user and track IDs for train data\n",
    "user_encoder = LabelEncoder()\n",
    "track_encoder = LabelEncoder()\n",
    "\n",
    "# Fit encoders on training data only\n",
    "user_encoder.fit(train_events['user_id'].to_numpy())\n",
    "track_encoder.fit(train_events['track_id'].to_numpy())\n",
    "\n",
    "# Transform training events\n",
    "train_events_encoded = train_events.with_columns([\n",
    "    pl.Series('user_idx', user_encoder.transform(train_events['user_id'].to_numpy())),\n",
    "    pl.Series('track_idx', track_encoder.transform(train_events['track_id'].to_numpy()))\n",
    "])\n",
    "\n",
    "print(f\"Encoded training data shape: {train_events_encoded.shape}\")\n",
    "print(f\"Unique users: {train_events_encoded['user_idx'].n_unique()}\")\n",
    "print(f\"Unique tracks: {train_events_encoded['track_idx'].n_unique()}\")\n",
    "\n",
    "# Create sparse user-track matrix (CSR format for efficiency)\n",
    "user_track_sparse = scipy.sparse.coo_matrix(\n",
    "    (\n",
    "        np.log1p(train_events_encoded['listen_count'].to_numpy()),  # log-scaled weights\n",
    "        (\n",
    "            train_events_encoded['user_idx'].to_numpy(),  # row indices\n",
    "            train_events_encoded['track_idx'].to_numpy()   # col indices\n",
    "        )\n",
    "    ),\n",
    "    shape=(\n",
    "        train_events_encoded['user_idx'].max() + 1,\n",
    "        train_events_encoded['track_idx'].max() + 1\n",
    "    )\n",
    ").tocsr()\n",
    "\n",
    "print(f\"\\nSparse matrix shape: {user_track_sparse.shape}\")\n",
    "print(f\"Sparsity: {1 - user_track_sparse.nnz / (user_track_sparse.shape[0] * user_track_sparse.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Train ALS Model\n",
    "\n",
    "# Initialize ALS model\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=64,              # Number of latent factors\n",
    "    regularization=0.01,     # L2 regularization\n",
    "    iterations=15,           # Number of training iterations\n",
    "    calculate_training_loss=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train the model (items x users matrix for implicit library)\n",
    "print(\"Training ALS model...\")\n",
    "als_model.fit(user_track_sparse.T.tocsr(), show_progress=True)\n",
    "\n",
    "print(\"\\n✓ ALS model trained successfully\")\n",
    "print(f\"Factors: {als_model.factors}\")\n",
    "print(f\"User factors shape: {als_model.user_factors.shape}\")\n",
    "print(f\"Item factors shape: {als_model.item_factors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Generate Personal Recommendations\n",
    "\n",
    "def get_personal_recommendations(\n",
    "    user_id, \n",
    "    als_model, \n",
    "    user_encoder, \n",
    "    track_encoder, \n",
    "    user_track_sparse,\n",
    "    items,\n",
    "    n_recommendations=10,\n",
    "    filter_already_listened=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Get personalized recommendations for a user using ALS model.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: original user ID\n",
    "    - als_model: trained ALS model\n",
    "    - user_encoder: fitted LabelEncoder for users\n",
    "    - track_encoder: fitted LabelEncoder for tracks\n",
    "    - user_track_sparse: sparse user-track matrix\n",
    "    - items: items DataFrame with track information\n",
    "    - n_recommendations: number of recommendations to return\n",
    "    - filter_already_listened: whether to filter out already listened tracks\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with recommended tracks and scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Encode user_id\n",
    "        user_idx = user_encoder.transform([user_id])[0]\n",
    "    except ValueError:\n",
    "        # User not in training set (cold start)\n",
    "        print(f\"User {user_id} not found in training data. Returning popular tracks.\")\n",
    "        return get_popular_tracks(train_events, items, top_n=n_recommendations)\n",
    "    \n",
    "    # Get recommendations from ALS\n",
    "    track_ids, scores = als_model.recommend(\n",
    "        user_idx,\n",
    "        user_track_sparse[user_idx],\n",
    "        N=n_recommendations + 100,  # Get more to filter out listened tracks\n",
    "        filter_already_liked_items=filter_already_listened\n",
    "    )\n",
    "    \n",
    "    # Decode track indices to original track IDs\n",
    "    recommended_track_ids = track_encoder.inverse_transform(track_ids)\n",
    "    \n",
    "    # Create DataFrame with recommendations\n",
    "    recommendations = pl.DataFrame({\n",
    "        'track_id': recommended_track_ids[:n_recommendations],\n",
    "        'score': scores[:n_recommendations]\n",
    "    })\n",
    "    \n",
    "    # Join with items to get track details\n",
    "    recommendations_with_info = recommendations.join(\n",
    "        items.select(['track_id', 'albums', 'artists', 'genres']),\n",
    "        on='track_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return recommendations_with_info\n",
    "\n",
    "\n",
    "# Test recommendations for a few users\n",
    "test_user_ids = train_events['user_id'].unique().head(5).to_list()\n",
    "\n",
    "for user_id in test_user_ids:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Recommendations for User ID: {user_id}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    recs = get_personal_recommendations(\n",
    "        user_id=user_id,\n",
    "        als_model=als_model,\n",
    "        user_encoder=user_encoder,\n",
    "        track_encoder=track_encoder,\n",
    "        user_track_sparse=user_track_sparse,\n",
    "        items=items,\n",
    "        n_recommendations=10\n",
    "    )\n",
    "    \n",
    "    print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Generate Recommendations for All Users\n",
    "\n",
    "def generate_all_recommendations(\n",
    "    user_ids,\n",
    "    als_model,\n",
    "    user_encoder,\n",
    "    track_encoder,\n",
    "    user_track_sparse,\n",
    "    items,\n",
    "    top_popular,\n",
    "    n_recommendations=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users (with fallback to popular for cold start).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with user_id, track_id, score, rank\n",
    "    \"\"\"\n",
    "    all_recommendations = []\n",
    "    \n",
    "    for i, user_id in enumerate(user_ids):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing user {i}/{len(user_ids)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to get personalized recommendations\n",
    "            user_idx = user_encoder.transform([user_id])[0]\n",
    "            track_ids, scores = als_model.recommend(\n",
    "                user_idx,\n",
    "                user_track_sparse[user_idx],\n",
    "                N=n_recommendations,\n",
    "                filter_already_liked_items=True\n",
    "            )\n",
    "            recommended_track_ids = track_encoder.inverse_transform(track_ids)\n",
    "            \n",
    "        except (ValueError, IndexError):\n",
    "            # Cold start user - use popular tracks\n",
    "            recommended_track_ids = top_popular['track_id'].head(n_recommendations).to_numpy()\n",
    "            scores = top_popular['popularity_score'].head(n_recommendations).to_numpy()\n",
    "        \n",
    "        # Create recommendation records\n",
    "        for rank, (track_id, score) in enumerate(zip(recommended_track_ids, scores), 1):\n",
    "            all_recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'track_id': track_id,\n",
    "                'score': score,\n",
    "                'rank': rank\n",
    "            })\n",
    "    \n",
    "    return pl.DataFrame(all_recommendations)\n",
    "\n",
    "\n",
    "# Generate recommendations for all test users\n",
    "print(\"Generating recommendations for test set users...\")\n",
    "test_user_ids = test_events['user_id'].unique().sort().to_list()\n",
    "\n",
    "test_recommendations = generate_all_recommendations(\n",
    "    user_ids=test_user_ids,\n",
    "    als_model=als_model,\n",
    "    user_encoder=user_encoder,\n",
    "    track_encoder=track_encoder,\n",
    "    user_track_sparse=user_track_sparse,\n",
    "    items=items,\n",
    "    top_popular=top_popular,\n",
    "    n_recommendations=10\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(test_recommendations):,} recommendations\")\n",
    "print(f\"For {test_recommendations['user_id'].n_unique():,} users\")\n",
    "print(test_recommendations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e18c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Save ALS Model and Encoders\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Create encoder directory\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "\n",
    "# Save encoders and model\n",
    "joblib.dump(user_encoder, os.path.join(encoder_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(track_encoder, os.path.join(encoder_dir, 'track_encoder.pkl'))\n",
    "joblib.dump(als_model, os.path.join(encoder_dir, 'als_model.pkl'))\n",
    "\n",
    "# Save popular tracks for cold start\n",
    "top_popular.write_parquet(os.path.join(preprocessed_dir, 'popular_tracks.parquet'))\n",
    "\n",
    "# Save recommendations\n",
    "test_recommendations.write_parquet(os.path.join(preprocessed_dir, 'test_recommendations.parquet'))\n",
    "\n",
    "print(\"✓ Saved:\")\n",
    "print(f\"  - user_encoder.pkl\")\n",
    "print(f\"  - track_encoder.pkl\")\n",
    "print(f\"  - als_model.pkl\")\n",
    "print(f\"  - popular_tracks.parquet\")\n",
    "print(f\"  - test_recommendations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09dc7e-7c91-4355-860a-b9cfb9f33f15",
   "metadata": {},
   "source": [
    "# Похожие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb683-b440-40a8-9975-894156a53872",
   "metadata": {},
   "source": [
    "Рассчитаем похожие, они позже пригодятся для онлайн-рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d07ee-4b12-4ce5-aa85-e45cb7a7a4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce370904-4c49-4152-8706-416074ea9b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0087a3e7-ca9f-42cd-944c-944222c1baef",
   "metadata": {},
   "source": [
    "# Построение признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a32e1-b90b-4eaf-9439-fc8deab9f34b",
   "metadata": {},
   "source": [
    "Построим три признака, можно больше, для ранжирующей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4ae84-406a-44a4-abec-4f80f93e3004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f84c35-f544-4c3d-ad53-9b1d2b684c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bcf88d-b236-46f0-a6f3-38ddd64895fe",
   "metadata": {},
   "source": [
    "# Ранжирование рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd8223-3418-4493-8c87-1f76286ebda0",
   "metadata": {},
   "source": [
    "Построим ранжирующую модель, чтобы сделать рекомендации более точными. Отранжируем рекомендации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1dd92-32a9-463d-827e-8bb9ee5bbb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe4db4-1ac5-44da-a13c-8e7f9768ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c84071-45b5-4a15-a683-e0ab034a3128",
   "metadata": {},
   "source": [
    "# Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b107fe4-554e-42b1-87d9-c435a52bb77a",
   "metadata": {},
   "source": [
    "Проверим оценку качества трёх типов рекомендаций: \n",
    "\n",
    "- топ популярных,\n",
    "- персональных, полученных при помощи ALS,\n",
    "- итоговых\n",
    "  \n",
    "по четырем метрикам: recall, precision, coverage, novelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6f388-aecb-443e-8647-14014e932d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df411f-14c1-4848-8797-f37afe449cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c8d38c-32b0-46a4-96f0-cd01dac708bc",
   "metadata": {},
   "source": [
    "# === Выводы, метрики ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7d5d8-7d1e-4fdf-a6cd-83e5ce92c684",
   "metadata": {},
   "source": [
    "Основные выводы при работе над расчётом рекомендаций, рассчитанные метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403097d-db36-46d9-8952-613c9bd51b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cfdd5-6f2e-4de6-8666-85804c87d04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
