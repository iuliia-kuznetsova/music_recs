{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a4ab1c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d04e7-1b0b-4e4a-9ddf-4526d7fef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf14d14",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d99d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get project root (one level up from notebooks/)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..')) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "\n",
    "# Get preprocessed directory from env or use default\n",
    "preprocessed_dir = os.getenv('PREPROCESSED_DATA_DIR', 'data/preprocessed')\n",
    "\n",
    "# If path is relative, make it relative to project root\n",
    "if not os.path.isabs(preprocessed_dir):\n",
    "    preprocessed_dir = os.path.join(project_root, preprocessed_dir)\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "print(f'üìÅ Project root: {project_root}')\n",
    "print(f'üìÅ Preprocessed dir: {preprocessed_dir}')\n",
    "\n",
    "if not os.path.exists(preprocessed_dir):\n",
    "    print(f'\\n‚ö†Ô∏è  Warning: Preprocessed directory not found!')\n",
    "    print('   Run from project root: python3 -m src.main')\n",
    "else:\n",
    "    print(f'\\n‚úÖ Preprocessed directory found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80fbc5-b660-4fac-8fbb-a5cae77313b3",
   "metadata": {},
   "source": [
    "## Preprocessed Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load datasets ---------- #\n",
    "\n",
    "# Check if preprocessed files exist\n",
    "items_path = os.path.join(preprocessed_dir, 'items.parquet')\n",
    "events_path = os.path.join(preprocessed_dir, 'events.parquet')\n",
    "tracks_catalog_path = os.path.join(preprocessed_dir, 'tracks_catalog_clean.parquet')\n",
    "\n",
    "if not os.path.exists(items_path) or not os.path.exists(events_path):\n",
    "    print('‚ùå Preprocessed data not found!')\n",
    "    print(f'   Looking for files in: {preprocessed_dir}')\n",
    "    print('\\nüìù To generate preprocessed data, run from terminal:')\n",
    "    print('   cd /home/mle-user/mle_projects/mle-project-sprint-4')\n",
    "    print('   python3 -m src.main')\n",
    "    print('\\n   This will:')\n",
    "    print('   1. Download raw data (if needed)')\n",
    "    print('   2. Preprocess and clean the data')\n",
    "    print('   3. Generate items.parquet and events.parquet')\n",
    "    raise FileNotFoundError(f'Preprocessed data not found in {preprocessed_dir}')\n",
    "\n",
    "# Load preprocessed data\n",
    "print('‚úÖ Loading preprocessed data...')\n",
    "items = pl.read_parquet(items_path)\n",
    "events = pl.read_parquet(events_path)\n",
    "tracks_catalog_clean = pl.read_parquet(tracks_catalog_path)\n",
    "\n",
    "print(f'\\n‚úÖ Data loaded successfully!')\n",
    "print(f'   Items: {items.shape}')\n",
    "print(f'   Events: {events.shape}')\n",
    "print(f'   Tracks catalog: {tracks_catalog_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e73960-fd38-4e15-8db0-9a25c35dfd25",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Check data summary ---------- #\n",
    "\n",
    "def data_summary(df: pl.DataFrame, name: str):\n",
    "    '''\n",
    "    Display a quick overview of a Polars DataFrame.\n",
    "    '''\n",
    "\n",
    "    print(f'\\n===== {name.upper()} =====')  \n",
    "  \n",
    "    # Sample rows\n",
    "    print('\\nSample rows:')\n",
    "    display(df.head())\n",
    "\n",
    "    # Shape\n",
    "    rows, cols = df.shape\n",
    "    print(f'\\nShape: {rows:,} rows x {cols} columns')\n",
    "    \n",
    "    # Data info\n",
    "    print('\\nSummary for numeric columns:')\n",
    "    display(df.describe())\n",
    "   \n",
    "    # Column info\n",
    "    print('\\nColumn names and types:')\n",
    "    for col in df.columns:\n",
    "        print(f'  {col}: {df[col].dtype}')\n",
    "    \n",
    "    # Missing values\n",
    "    print('\\nMissing values:')\n",
    "    display(df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Items data summary ---------- #\n",
    "\n",
    "data_summary(items, 'items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Events data summary ---------- #\n",
    "\n",
    "data_summary(events, 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks_catalog_clean already loaded in Cell 5\n",
    "display(tracks_catalog_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed - duplicate of previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- Top tracks by popularity ---------- #\n",
    "\n",
    "top_tracks_by_listen_number = (\n",
    "    events\n",
    "        .group_by('track_id')\n",
    "        .agg(pl.sum('listen_count').alias('total_listen_count'))\n",
    "        .join(tracks_catalog_clean.select(['track_id', 'track_clean']), on='track_id', how='left')\n",
    "        .sort('total_listen_count', descending=True)\n",
    "        .head(10)\n",
    ")\n",
    "\n",
    "display(top_tracks_by_listen_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient approach: aggregate first (small result), then join for names\n",
    "print(\"=== Top Track Versions ===\")\n",
    "\n",
    "# Step 1: Get top 10 track IDs by listen count (small DataFrame)\n",
    "top_track_ids = (\n",
    "    events\n",
    "    .lazy()\n",
    "    .group_by('track_id')\n",
    "    .agg(pl.sum('listen_count').alias('total_listens'))\n",
    "    .sort('total_listens', descending=False)\n",
    "    .tail(10)  # Use tail for descending (more efficient)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Step 2: Join only these 10 tracks with catalog (small join)\n",
    "top_track_versions = (\n",
    "    top_track_ids\n",
    "    .join(tracks_catalog_clean.select(['track_id', 'track_clean', 'track_group_id']), \n",
    "          on='track_id', how='left')\n",
    "    .sort('total_listens', descending=True)\n",
    ")\n",
    "display(top_track_versions)\n",
    "\n",
    "# 2. Most popular songs (versions aggregated) - memory efficient\n",
    "print(\"\\n=== Top Songs (All Versions Combined) ===\")\n",
    "\n",
    "# Step 1: Aggregate by track_id first (reduces size before join)\n",
    "events_agg = (\n",
    "    events\n",
    "    .lazy()\n",
    "    .group_by('track_id')\n",
    "    .agg(pl.sum('listen_count').alias('track_listens'))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Step 2: Join with catalog (smaller join)\n",
    "events_with_groups = (\n",
    "    events_agg\n",
    "    .join(tracks_catalog_clean.select(['track_id', 'track_clean', 'track_group_id']), \n",
    "          on='track_id', how='left')\n",
    ")\n",
    "\n",
    "# Step 3: Group by track_group_id\n",
    "top_songs = (\n",
    "    events_with_groups\n",
    "    .group_by(['track_group_id', 'track_clean'])\n",
    "    .agg([\n",
    "        pl.sum('track_listens').alias('total_listens'),\n",
    "        pl.len().alias('num_versions'),\n",
    "    ])\n",
    "    .sort('total_listens', descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "display(top_songs)\n",
    "\n",
    "# 3. Analysis: How many top tracks are just different versions?\n",
    "print(\"\\n=== Diversity Analysis ===\")\n",
    "\n",
    "# Get top 100 tracks\n",
    "top_100_track_ids = (\n",
    "    events\n",
    "    .lazy()\n",
    "    .group_by('track_id')\n",
    "    .agg(pl.sum('listen_count').alias('total_listens'))\n",
    "    .sort('total_listens', descending=False)\n",
    "    .tail(100)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "top_100_with_groups = (\n",
    "    top_100_track_ids\n",
    "    .join(tracks_catalog_clean.select(['track_id', 'track_group_id']), \n",
    "          on='track_id', how='left')\n",
    ")\n",
    "\n",
    "unique_groups_in_top_100 = top_100_with_groups['track_group_id'].n_unique()\n",
    "print(f\"Top 100 tracks represent {unique_groups_in_top_100} unique songs\")\n",
    "print(f\"Duplication rate: {(100 - unique_groups_in_top_100) / 100:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c32a5a-d3be-4f96-8dd9-f7860951020c",
   "metadata": {},
   "source": [
    "–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∂–∞–Ω—Ä—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 genres by listening number - memory efficient\n",
    "# Step 1: Aggregate events by track_id (reduces size)\n",
    "events_by_track = (\n",
    "    events\n",
    "    .lazy()\n",
    "    .group_by('track_id')\n",
    "    .agg(pl.sum('listen_count').alias('track_listen_count'))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Step 2: Get unique track-genre mapping (small subset of items)\n",
    "track_genres = items.select(['track_id', 'genre_clean']).unique(['track_id', 'genre_clean'])\n",
    "\n",
    "# Step 3: Join and aggregate\n",
    "genres_by_listen_count = (\n",
    "    events_by_track\n",
    "    .join(track_genres, on='track_id', how='left')\n",
    "    .group_by('genre_clean')\n",
    "    .agg(pl.sum('track_listen_count').alias('total_listen_count'))\n",
    "    .sort('total_listen_count', descending=True)\n",
    ")\n",
    "\n",
    "top_5_genres = genres_by_listen_count.head(5)\n",
    "display(top_5_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15254e66-e80e-473b-ba24-abebea5ccac7",
   "metadata": {},
   "source": [
    "–¢—Ä–µ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∏–∫—Ç–æ –Ω–µ –ø—Ä–æ—Å–ª—É—à–∞–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54020fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks that haven't been listened to by anybody - memory efficient\n",
    "# Step 1: Get set of listened track IDs (small set)\n",
    "listened_track_ids = set(events['track_id'].unique())\n",
    "\n",
    "# Step 2: Get unique tracks from items\n",
    "unique_tracks = items.select(['track_id', 'track_clean', 'artist_clean', 'album_clean', 'genre_clean']).unique('track_id')\n",
    "\n",
    "# Step 3: Filter out listened tracks (using Python set for efficiency)\n",
    "unlistened_tracks = unique_tracks.filter(~pl.col('track_id').is_in(list(listened_track_ids)))\n",
    "\n",
    "print(f'Number of unlistened tracks: {unlistened_tracks.height:,}')\n",
    "display(unlistened_tracks.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
