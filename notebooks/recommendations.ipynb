{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4095ae4-7294-4b28-853e-88d235002c97",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2318d2-9df8-4911-915e-15b725c44f8a",
   "metadata": {},
   "source": [
    "Загружаем библиотеки необходимые для выполнения кода ноутбука."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4ab1c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662d04e7-1b0b-4e4a-9ddf-4526d7fef119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle-user/mle_projects/mle-project-sprint-4/.venv_recsys/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "#import calendar\n",
    "import joblib\n",
    "#import s3fs\n",
    "import gc\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf14d14",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d99d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "datasets = {\n",
    "    'tracks.parquet': os.getenv('RAW_URL_TRACKS'),\n",
    "    'catalog_names.parquet': os.getenv('RAW_URL_CATALOG_NAMES'),\n",
    "    'interactions.parquet': os.getenv('RAW_URL_INTERACTIONS'),\n",
    "}\n",
    "\n",
    "raw_dir = os.getenv('RAW_DATA_DIR', '../data/raw')\n",
    "preprocessed_dir = os.getenv('PREPROCESSED_DATA_DIR', '../data/preprocessed')\n",
    "encoder_dir = os.getenv('ENCODERS_DIR', '../encoders')\n",
    "\n",
    "s3_bucket = os.getenv('S3_BUCKET_NAME')\n",
    "s3_prefix = os.getenv('S3_PREFIX', 'recsys/data/')\n",
    "s3_region = os.getenv('S3_REGION', 'us-east-1')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80fbc5-b660-4fac-8fbb-a5cae77313b3",
   "metadata": {},
   "source": [
    "# === ЭТАП 1 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263a8b3-fe99-4204-8a2e-105182792c11",
   "metadata": {},
   "source": [
    "# Загрузка первичных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54a6a5-1656-4e3c-99d1-49dc39451d33",
   "metadata": {},
   "source": [
    "Загружаем первичные данные из файлов:\n",
    "- tracks.parquet\n",
    "- catalog_names.parquet\n",
    "- interactions.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b8961-3f35-4e58-9d6b-3e2dbd2c4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Download datasets and save locally ---------- #\n",
    "# Create directory\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Download and save each dataset\n",
    "for filename, url in datasets.items():\n",
    "    save_path = os.path.join(raw_dir, filename)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f'Saved {filename} to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load datasets ---------- #\n",
    "tracks = pl.read_parquet(os.path.join(raw_dir, 'tracks.parquet'))\n",
    "catalog_names = pl.read_parquet(os.path.join(raw_dir, 'catalog_names.parquet'))\n",
    "interactions = pl.read_parquet(os.path.join(raw_dir, 'interactions.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2a1f7-a05f-4f39-af90-5f4018aa6f9d",
   "metadata": {},
   "source": [
    "# Обзор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a85307-896c-4fac-9fcf-f0dffa90889e",
   "metadata": {},
   "source": [
    "Проверяем данные, есть ли с ними явные проблемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Check data summary ---------- #\n",
    "def data_summary(df: pd.DataFrame, name: str):\n",
    "    '''\n",
    "        Display a quick overview of a DataFrame.\n",
    "    '''\n",
    "\n",
    "    print(f'\\n===== {name.upper()} =====')  \n",
    "  \n",
    "    # Sample rows\n",
    "    print('\\nSample rows:')\n",
    "    display(df.head())\n",
    "\n",
    "    # Shape\n",
    "    rows, cols = df.shape\n",
    "    print(f'\\nShape: {rows} rows x {cols} columns')\n",
    "    \n",
    "    # Data info\n",
    "    print('\\nSummary for numeric columns:')\n",
    "    print(df.describe())\n",
    "   \n",
    "    # Unique values (column-wise, skip if error occurs)\n",
    "    print('\\nUnique values (for each column):')\n",
    "    try:\n",
    "        for col in df.columns:\n",
    "            print(f'\\nColumn: {col}')\n",
    "            print(df[col].value_counts())\n",
    "    except Exception as e:\n",
    "        print(f'Skipped value_counts due to error: {e}')\n",
    "    \n",
    "    # Missing values\n",
    "    print('\\nMissing values:')\n",
    "    print(df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(tracks, 'tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2e2eb-3bec-4ce1-87ac-232bab8bc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(catalog_names, 'catalog_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b06299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(interactions, 'interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba0a88",
   "metadata": {},
   "source": [
    "### Main takeaways\n",
    "1. Tracs dataframe contains lists instead of scalar values. This can cause several problems:\n",
    "- Unable to get insights on data;\n",
    "Counting, merging, or joining on list columns is tricky.\n",
    "- Missing values are hidden;\n",
    "df.isna().sum() can’t detect empty lists, so there might be tracks with no genres or artists but they look as not missing values.\n",
    "- Hard to work with for ML models.\n",
    "Most algorithms expect scalar values, not lists.\n",
    "Thus, it's necessary to explode the lists to get a dataframe with one row per track-per-item (artist, genre, album).\n",
    "\n",
    "2. Catalog_names dataframe is in a format where everything (tracks, albums, artists, genres) is stacked in one column, and the type column tells what each row represents. This format isn't convenient for futher work. Thus, it's necessary to split catalog_names into several dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659948f",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "**Note:** For large-scale processing, use `python3 -m src.main` from terminal instead of running cells below. The notebook cells are for exploration only and may cause memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Explode lists of tracs dataframe into separate rows ---------- #\n",
    "tracks_exploded = (\n",
    "    tracks\n",
    "        .explode('albums')\n",
    "        .explode('artists')\n",
    "        .explode('genres')\n",
    "        .rename({\n",
    "            'albums': 'album_id',\n",
    "            'artists': 'artist_id',\n",
    "            'genres': 'genre_id'\n",
    "        })\n",
    ")\n",
    "data_summary(tracks_exploded, 'tracks_exploded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper functions (imported from src/preprocess_data.py) ---------- #\n",
    "# These match the production pipeline logic\n",
    "\n",
    "UNKNOWN_TOKEN = 'Unknown'\n",
    "BRACKETS_PATTERN = r'[\\(\\[][^\\)\\]]*[\\)\\]]'\n",
    "FEATURE_PATTERN = r'\\b(feat\\.?|ft\\.?|featuring)\\b.*'\n",
    "VERSION_TAG_PATTERN = r'\\b(live|remix|extended|radio edit|acoustic|remastered(?:\\s+\\d{4})?)\\b'\n",
    "COVER_PATTERN = r'\\bcover(ed)?\\b.*'\n",
    "\n",
    "def basic_standardize(colname: str, alias: str) -> pl.Expr:\n",
    "    \"\"\"STEP 0: Basic text standardization.\"\"\"\n",
    "    return (\n",
    "        pl.col(colname)\n",
    "        .cast(pl.Utf8)\n",
    "        .fill_null('')\n",
    "        .str.normalize('NFKD')\n",
    "        .str.replace_all(r'[\\p{M}]', '')\n",
    "        .str.to_lowercase()\n",
    "        .str.replace_all(r'[^\\w\\s]', ' ')\n",
    "        .str.replace_all(r'\\s+', ' ')\n",
    "        .str.strip_chars()\n",
    "        .alias(alias)\n",
    "    )\n",
    "\n",
    "def clean_entity_name(std_col: str, alias: str, strip_versions: bool = False, strip_features: bool = False) -> pl.Expr:\n",
    "    \"\"\"STEP 3: Clean entity names.\"\"\"\n",
    "    expr = pl.col(std_col).fill_null('')\n",
    "    expr = expr.str.replace_all(BRACKETS_PATTERN, ' ')\n",
    "    if strip_features:\n",
    "        expr = expr.str.replace_all(FEATURE_PATTERN, ' ')\n",
    "    if strip_versions:\n",
    "        expr = expr.str.replace_all(VERSION_TAG_PATTERN, ' ')\n",
    "        expr = expr.str.replace_all(COVER_PATTERN, ' ')\n",
    "    expr = expr.str.replace_all(r'[^\\w\\s]', ' ')\n",
    "    expr = expr.str.replace_all(r'\\s+', ' ')\n",
    "    return expr.str.strip_chars().str.to_titlecase().alias(alias)\n",
    "\n",
    "def normalize_track_title(source_col: str, alias: str = 'title_normalized') -> pl.Expr:\n",
    "    \"\"\"STEP 4: Normalize track titles for grouping.\"\"\"\n",
    "    expr = pl.col(source_col).cast(pl.Utf8).fill_null('')\n",
    "    expr = expr.str.to_lowercase()\n",
    "    expr = expr.str.replace_all(BRACKETS_PATTERN, ' ')\n",
    "    expr = expr.str.replace_all(FEATURE_PATTERN, ' ')\n",
    "    expr = expr.str.replace_all(VERSION_TAG_PATTERN, ' ')\n",
    "    expr = expr.str.replace_all(COVER_PATTERN, ' ')\n",
    "    expr = expr.str.replace_all(r'[^\\w\\s]', ' ')\n",
    "    expr = expr.str.replace_all(r'\\s+', ' ')\n",
    "    return expr.str.strip_chars().alias(alias)\n",
    "\n",
    "def ensure_token(col: str, token: str = UNKNOWN_TOKEN) -> pl.Expr:\n",
    "    \"\"\"Fill empty strings with default token.\"\"\"\n",
    "    return (\n",
    "        pl.when(pl.col(col).is_null() | (pl.col(col).str.len_chars() == 0))\n",
    "        .then(pl.lit(token))\n",
    "        .otherwise(pl.col(col))\n",
    "        .alias(col)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115487bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 0: Basic standardization for catalog entities ---------- #\n",
    "\n",
    "tracks_catalog = (\n",
    "    catalog_names\n",
    "        .filter(pl.col('type') == 'track')\n",
    "        .select([\n",
    "            pl.col('id').alias('track_id'),\n",
    "            pl.col('name').alias('track_name')\n",
    "        ])\n",
    "        .with_columns([basic_standardize('track_name', 'track_name_std')])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('track_name_std') == '')\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('track_name_std'))\n",
    "              .alias('track_name_std')\n",
    "        ])\n",
    ")\n",
    "\n",
    "artists_catalog = (\n",
    "    catalog_names\n",
    "        .filter(pl.col('type') == 'artist')\n",
    "        .select([\n",
    "            pl.col('id').alias('artist_id'),\n",
    "            pl.col('name').alias('artist_name')\n",
    "        ])\n",
    "        .with_columns([basic_standardize('artist_name', 'artist_name_std')])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_name_std') == '')\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('artist_name_std'))\n",
    "              .alias('artist_name_std')\n",
    "        ])\n",
    ")\n",
    "\n",
    "albums_catalog = (\n",
    "    catalog_names\n",
    "        .filter(pl.col('type') == 'album')\n",
    "        .select([\n",
    "            pl.col('id').alias('album_id'),\n",
    "            pl.col('name').alias('album_name')\n",
    "        ])\n",
    "        .with_columns([basic_standardize('album_name', 'album_name_std')])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('album_name_std') == '')\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('album_name_std'))\n",
    "              .alias('album_name_std')\n",
    "        ])\n",
    ")\n",
    "\n",
    "genres_catalog = (\n",
    "    catalog_names\n",
    "        .filter(pl.col('type') == 'genre')\n",
    "        .select([\n",
    "            pl.col('id').alias('genre_id'),\n",
    "            pl.col('name').alias('genre_name')\n",
    "        ])\n",
    "        .with_columns([basic_standardize('genre_name', 'genre_name_std')])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('genre_name_std') == '')\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('genre_name_std'))\n",
    "              .alias('genre_name_std')\n",
    "        ])\n",
    ")\n",
    "\n",
    "data_summary(tracks_catalog, 'tracks_catalog_std')\n",
    "data_summary(artists_catalog, 'artists_catalog_std')\n",
    "data_summary(albums_catalog, 'albums_catalog_std')\n",
    "data_summary(genres_catalog, 'genres_catalog_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 1.1: Canonical artists (deduplicate entities) ---------- #\n",
    "\n",
    "artists_dedup = (\n",
    "    artists_catalog\n",
    "        .group_by('artist_name_std')\n",
    "        .agg(pl.min('artist_id').alias('artist_id_canonical'))\n",
    "        .with_columns([\n",
    "            clean_entity_name('artist_name_std', 'artist_clean')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_clean') == '')\n",
    "              .then(pl.lit('Unknown'))\n",
    "              .otherwise(pl.col('artist_clean'))\n",
    "              .alias('artist_clean')\n",
    "        ])\n",
    ")\n",
    "\n",
    "artist_id_map = (\n",
    "    artists_catalog\n",
    "        .join(\n",
    "            artists_dedup.select(['artist_name_std', 'artist_id_canonical']),\n",
    "            on='artist_name_std',\n",
    "            how='left'\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_id_canonical').is_null())\n",
    "              .then(pl.col('artist_id'))\n",
    "              .otherwise(pl.col('artist_id_canonical'))\n",
    "              .alias('artist_id_canonical')\n",
    "        ])\n",
    "        .select(['artist_id', 'artist_id_canonical'])\n",
    "        .unique()\n",
    ")\n",
    "\n",
    "data_summary(artists_dedup, 'artists_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 1.2: Canonical albums (deduplicate per artist) ---------- #\n",
    "\n",
    "album_artist_bridge = (\n",
    "    tracks_exploded\n",
    "        .select(['album_id', 'artist_id'])\n",
    "        .drop_nulls()\n",
    "        .unique()\n",
    "        .join(artist_id_map, on='artist_id', how='left')\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_id_canonical').is_null())\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('artist_id_canonical'))\n",
    "              .alias('artist_id_canonical')\n",
    "        ])\n",
    "        .select(['album_id', 'artist_id_canonical'])\n",
    ")\n",
    "\n",
    "albums_with_artist = (\n",
    "    albums_catalog\n",
    "        .join(album_artist_bridge, on='album_id', how='left')\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_id_canonical').is_null())\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('artist_id_canonical'))\n",
    "              .alias('artist_id_canonical')\n",
    "        ])\n",
    ")\n",
    "\n",
    "albums_dedup = (\n",
    "    albums_with_artist\n",
    "        .group_by(['album_name_std', 'artist_id_canonical'])\n",
    "        .agg(pl.min('album_id').alias('album_id_canonical'))\n",
    "        .with_columns([\n",
    "            clean_entity_name('album_name_std', 'album_clean', strip_versions=True, strip_features=True)\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('album_clean') == '')\n",
    "              .then(pl.lit('Unknown'))\n",
    "              .otherwise(pl.col('album_clean'))\n",
    "              .alias('album_clean')\n",
    "        ])\n",
    ")\n",
    "\n",
    "album_id_map = (\n",
    "    albums_with_artist\n",
    "        .join(\n",
    "            albums_dedup.select(['album_name_std', 'artist_id_canonical', 'album_id_canonical']),\n",
    "            on=['album_name_std', 'artist_id_canonical'],\n",
    "            how='left'\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('album_id_canonical').is_null())\n",
    "              .then(pl.col('album_id'))\n",
    "              .otherwise(pl.col('album_id_canonical'))\n",
    "              .alias('album_id_canonical')\n",
    "        ])\n",
    "        .select(['album_id', 'album_id_canonical'])\n",
    "        .unique()\n",
    ")\n",
    "\n",
    "data_summary(albums_dedup, 'albums_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f45546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 1.3: Canonical tracks (per artist) ---------- #\n",
    "\n",
    "track_artist_bridge = (\n",
    "    tracks_exploded\n",
    "        .select(['track_id', 'artist_id'])\n",
    "        .drop_nulls()\n",
    "        .unique()\n",
    "        .join(artist_id_map, on='artist_id', how='left')\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_id_canonical').is_null())\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('artist_id_canonical'))\n",
    "              .alias('artist_id_canonical')\n",
    "        ])\n",
    "        .select(['track_id', 'artist_id_canonical'])\n",
    ")\n",
    "\n",
    "tracks_with_artist = (\n",
    "    tracks_catalog\n",
    "        .join(track_artist_bridge, on='track_id', how='left')\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('artist_id_canonical').is_null())\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('artist_id_canonical'))\n",
    "              .alias('artist_id_canonical')\n",
    "        ])\n",
    ")\n",
    "\n",
    "tracks_dedup = (\n",
    "    tracks_with_artist\n",
    "        .group_by(['track_name_std', 'artist_id_canonical'])\n",
    "        .agg(pl.min('track_id').alias('track_id_canonical'))\n",
    "        .with_columns([\n",
    "            clean_entity_name('track_name_std', 'track_clean', strip_versions=True, strip_features=True)\n",
    "        ])\n",
    "        .with_columns([\n",
    "            normalize_track_title('track_clean', 'title_normalized')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('track_clean') == '')\n",
    "              .then(pl.lit('Unknown'))\n",
    "              .otherwise(pl.col('track_clean'))\n",
    "              .alias('track_clean'),\n",
    "            pl.when(pl.col('title_normalized') == '')\n",
    "              .then(pl.lit(UNKNOWN_TOKEN))\n",
    "              .otherwise(pl.col('title_normalized'))\n",
    "              .alias('title_normalized')\n",
    "        ])\n",
    ")\n",
    "\n",
    "track_group_lookup = (\n",
    "    tracks_dedup\n",
    "        .group_by(['title_normalized', 'artist_id_canonical'])\n",
    "        .agg(pl.min('track_id_canonical').alias('track_group_id'))\n",
    ")\n",
    "\n",
    "tracks_dedup = (\n",
    "    tracks_dedup\n",
    "        .join(track_group_lookup, on=['title_normalized', 'artist_id_canonical'], how='left')\n",
    ")\n",
    "\n",
    "track_id_map = (\n",
    "    tracks_with_artist\n",
    "        .join(\n",
    "            tracks_dedup.select(['track_name_std', 'artist_id_canonical', 'track_id_canonical']),\n",
    "            on=['track_name_std', 'artist_id_canonical'],\n",
    "            how='left'\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('track_id_canonical').is_null())\n",
    "              .then(pl.col('track_id'))\n",
    "              .otherwise(pl.col('track_id_canonical'))\n",
    "              .alias('track_id_canonical')\n",
    "        ])\n",
    "        .select(['track_id', 'track_id_canonical'])\n",
    "        .unique()\n",
    ")\n",
    "\n",
    "data_summary(tracks_dedup, 'tracks_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 1.4: Canonical genres ---------- #\n",
    "\n",
    "genres_dedup = (\n",
    "    genres_catalog\n",
    "        .group_by('genre_name_std')\n",
    "        .agg(pl.min('genre_id').alias('genre_id_canonical'))\n",
    "        .with_columns([\n",
    "            clean_entity_name('genre_name_std', 'genre_clean')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('genre_clean') == '')\n",
    "              .then(pl.lit('Unknown'))\n",
    "              .otherwise(pl.col('genre_clean'))\n",
    "              .alias('genre_clean')\n",
    "        ])\n",
    ")\n",
    "\n",
    "genre_id_map = (\n",
    "    genres_catalog\n",
    "        .join(\n",
    "            genres_dedup.select(['genre_name_std', 'genre_id_canonical']),\n",
    "            on='genre_name_std',\n",
    "            how='left'\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('genre_id_canonical').is_null())\n",
    "              .then(pl.col('genre_id'))\n",
    "              .otherwise(pl.col('genre_id_canonical'))\n",
    "              .alias('genre_id_canonical')\n",
    "        ])\n",
    "        .select(['genre_id', 'genre_id_canonical'])\n",
    "        .unique()\n",
    ")\n",
    "\n",
    "data_summary(genres_dedup, 'genres_dedup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 5–7: Canonical items dataframe (dedupe, drop nulls, validate) ---------- #\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(preprocessed_dir, 'checkpoints')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "FACT_IDS_PATH = os.path.join(CHECKPOINT_DIR, 'fact_ids_checkpoint.parquet')\n",
    "ITEMS_PATH = os.path.join(preprocessed_dir, 'items_canonical.parquet')\n",
    "CACHE_FACT_IDS = True\n",
    "CACHE_ITEMS = True\n",
    "\n",
    "# Phase 1: map raw IDs to canonical IDs using lightweight dict lookups\n",
    "track_lookup = dict(zip(track_id_map['track_id'], track_id_map['track_id_canonical']))\n",
    "artist_lookup = dict(zip(artist_id_map['artist_id'], artist_id_map['artist_id_canonical']))\n",
    "album_lookup = dict(zip(album_id_map['album_id'], album_id_map['album_id_canonical']))\n",
    "genre_lookup = dict(zip(genre_id_map['genre_id'], genre_id_map['genre_id_canonical']))\n",
    "\n",
    "lazy_fact_ids = (\n",
    "    tracks_exploded\n",
    "        .lazy()\n",
    "        .select(['track_id', 'artist_id', 'album_id', 'genre_id'])\n",
    "        .with_columns([\n",
    "            pl.col('track_id').replace(track_lookup, default=None).alias('track_id'),\n",
    "            pl.col('artist_id').replace(artist_lookup, default=None).alias('artist_id'),\n",
    "            pl.col('album_id').replace(album_lookup, default=None).alias('album_id'),\n",
    "            pl.col('genre_id').replace(genre_lookup, default=None).alias('genre_id')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(col).cast(pl.Int64).alias(col)\n",
    "            for col in ['track_id', 'artist_id', 'album_id', 'genre_id']\n",
    "        ])\n",
    "        .drop_nulls(['track_id', 'artist_id', 'album_id', 'genre_id'])\n",
    ")\n",
    "\n",
    "if CACHE_FACT_IDS:\n",
    "    fact_ids = lazy_fact_ids.collect(engine='streaming')\n",
    "    fact_ids.write_parquet(FACT_IDS_PATH)\n",
    "    del fact_ids\n",
    "    gc.collect()\n",
    "    fact_ids_lazy = pl.scan_parquet(FACT_IDS_PATH)\n",
    "else:\n",
    "    fact_ids_lazy = lazy_fact_ids\n",
    "\n",
    "# Phase 2: add cleaned names and group ids (dimensions are smaller)\n",
    "lazy_items = (\n",
    "    fact_ids_lazy\n",
    "        .join(\n",
    "            tracks_dedup.select(['track_id_canonical', 'track_clean', 'track_group_id']).lazy(),\n",
    "            left_on='track_id',\n",
    "            right_on='track_id_canonical',\n",
    "            how='left'\n",
    "        )\n",
    "        .join(\n",
    "            artists_dedup.select(['artist_id_canonical', 'artist_clean']).lazy(),\n",
    "            left_on='artist_id',\n",
    "            right_on='artist_id_canonical',\n",
    "            how='left'\n",
    "        )\n",
    "        .join(\n",
    "            albums_dedup.select(['album_id_canonical', 'album_clean']).lazy(),\n",
    "            left_on='album_id',\n",
    "            right_on='album_id_canonical',\n",
    "            how='left'\n",
    "        )\n",
    "        .join(\n",
    "            genres_dedup.select(['genre_id_canonical', 'genre_clean']).lazy(),\n",
    "            left_on='genre_id',\n",
    "            right_on='genre_id_canonical',\n",
    "            how='left'\n",
    "        )\n",
    "        .select([\n",
    "            pl.col('track_id').alias('track_id'),\n",
    "            'track_clean',\n",
    "            'track_group_id',\n",
    "            pl.col('artist_id').alias('artist_id'),\n",
    "            'artist_clean',\n",
    "            pl.col('album_id').alias('album_id'),\n",
    "            'album_clean',\n",
    "            pl.col('genre_id').alias('genre_id'),\n",
    "            'genre_clean'\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('track_group_id').is_null())\n",
    "              .then(pl.col('track_id'))\n",
    "              .otherwise(pl.col('track_group_id'))\n",
    "              .alias('track_group_id')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(col).is_null() | (pl.col(col).str.len_chars() == 0))\n",
    "              .then(pl.lit('Unknown'))\n",
    "              .otherwise(pl.col(col))\n",
    "              .alias(col)\n",
    "            for col in ['track_clean', 'artist_clean', 'album_clean', 'genre_clean']\n",
    "        ])\n",
    ")\n",
    "\n",
    "if CACHE_ITEMS:\n",
    "    items_df = (\n",
    "        lazy_items\n",
    "            .collect(engine='streaming')\n",
    "            .unique(subset=['track_id', 'artist_id', 'album_id', 'genre_id'])\n",
    "    )\n",
    "    items_df.write_parquet(ITEMS_PATH)\n",
    "else:\n",
    "    items_df = (\n",
    "        lazy_items\n",
    "            .collect(engine='streaming')\n",
    "            .unique(subset=['track_id', 'artist_id', 'album_id', 'genre_id'])\n",
    "    )\n",
    "\n",
    "data_summary(items_df, 'items_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- STEP 8: Final canonical catalog tables ---------- #\n",
    "\n",
    "tracks_catalog_clean = (\n",
    "    items_df\n",
    "        .group_by('track_id')\n",
    "        .agg([\n",
    "            pl.first('track_clean').alias('track_clean'),\n",
    "            pl.first('track_group_id').alias('track_group_id'),\n",
    "            pl.first('artist_id').alias('artist_id'),\n",
    "            pl.first('album_id').alias('album_id'),\n",
    "            pl.first('genre_id').alias('genre_id')\n",
    "        ])\n",
    ")\n",
    "\n",
    "artists_catalog_clean = (\n",
    "    artists_dedup\n",
    "        .select([\n",
    "            pl.col('artist_id_canonical').alias('artist_id'),\n",
    "            pl.col('artist_clean')\n",
    "        ])\n",
    "        .unique('artist_id')\n",
    ")\n",
    "\n",
    "albums_catalog_clean = (\n",
    "    albums_dedup\n",
    "        .select([\n",
    "            pl.col('album_id_canonical').alias('album_id'),\n",
    "            pl.col('album_clean'),\n",
    "            pl.col('artist_id_canonical').alias('artist_id')\n",
    "        ])\n",
    "        .unique('album_id')\n",
    ")\n",
    "\n",
    "genres_catalog_clean = (\n",
    "    genres_dedup\n",
    "        .select([\n",
    "            pl.col('genre_id_canonical').alias('genre_id'),\n",
    "            pl.col('genre_clean')\n",
    "        ])\n",
    "        .unique('genre_id')\n",
    ")\n",
    "\n",
    "data_summary(tracks_catalog_clean, 'tracks_catalog_clean')\n",
    "data_summary(artists_catalog_clean, 'artists_catalog_clean')\n",
    "data_summary(albums_catalog_clean, 'albums_catalog_clean')\n",
    "data_summary(genres_catalog_clean, 'genres_catalog_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac55c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loading preprocessed data from disk...\n",
      "Items: (5606516, 9)\n",
      "Events: (213656164, 4)\n",
      "All catalog tables loaded.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load preprocessed data (recommended approach) ---------- #\n",
    "# If you ran: python3 -m src.main\n",
    "# Load the outputs directly instead of running memory-heavy cells above\n",
    "\n",
    "items_path = os.path.join(preprocessed_dir, 'items.parquet')\n",
    "events_path = os.path.join(preprocessed_dir, 'events.parquet')\n",
    "\n",
    "if os.path.exists(items_path) and os.path.exists(events_path):\n",
    "    print('✓ Loading preprocessed data from disk...')\n",
    "    items = pl.read_parquet(items_path)\n",
    "    events = pl.read_parquet(events_path)\n",
    "    \n",
    "    tracks_catalog_clean = pl.read_parquet(os.path.join(preprocessed_dir, 'tracks_catalog_clean.parquet'))\n",
    "    artists_catalog_clean = pl.read_parquet(os.path.join(preprocessed_dir, 'artists_catalog_clean.parquet'))\n",
    "    albums_catalog_clean = pl.read_parquet(os.path.join(preprocessed_dir, 'albums_catalog_clean.parquet'))\n",
    "    genres_catalog_clean = pl.read_parquet(os.path.join(preprocessed_dir, 'genres_catalog_clean.parquet'))\n",
    "    \n",
    "    print(f'Items: {items.shape}')\n",
    "    print(f'Events: {events.shape}')\n",
    "    print('All catalog tables loaded.')\n",
    "else:\n",
    "    print('⚠ Preprocessed data not found.')\n",
    "    print('Run from terminal: python3 -m src.main --raw-dir data/raw --preprocessed-dir data/preprocessed')\n",
    "    print('Or execute preprocessing cells above (may cause memory issues on this VM)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e378e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problematic rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a regex pattern for \"safe\" characters: letters, numbers, whitespace\n",
    "safe_pattern = r'^[\\w\\s]+$'\n",
    "\n",
    "# Filter rows where any column contains characters outside the safe pattern\n",
    "problematic_rows = items.filter(\n",
    "    ~pl.col('track_clean').str.contains(safe_pattern) |\n",
    "    ~pl.col('artist_clean').str.contains(safe_pattern) |\n",
    "    ~pl.col('album_clean').str.contains(safe_pattern) |\n",
    "    ~pl.col('genre_clean').str.contains(safe_pattern)\n",
    ")\n",
    "\n",
    "print(f'Number of problematic rows: {problematic_rows.height}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07cf3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>track_id</th><th>track_clean</th><th>track_group_id</th><th>artist_id</th><th>artist_clean</th><th>album_id</th><th>album_clean</th><th>genre_id</th><th>genre_clean</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>15421156</td><td>&quot;Saint Saens Le Carnaval Des An…</td><td>15421156</td><td>378040</td><td>&quot;Pascal Roge&quot;</td><td>10515261</td><td>&quot;Classical 100&quot;</td><td>69</td><td>&quot;Classicalmusic&quot;</td></tr><tr><td>15431752</td><td>&quot;Verdi Rigoletto Act Ii Cortigi…</td><td>15431752</td><td>83201</td><td>&quot;Wiener Philharmoniker&quot;</td><td>31947</td><td>&quot;Verdi Rigoletto&quot;</td><td>117</td><td>&quot;Classicalmasterpieces&quot;</td></tr><tr><td>15432989</td><td>&quot;Vivaldi The Four Seasons Winte…</td><td>15432989</td><td>87464</td><td>&quot;Albrecht Mayer&quot;</td><td>13427069</td><td>&quot;In The Mood For Vivaldi&quot;</td><td>69</td><td>&quot;Classicalmusic&quot;</td></tr><tr><td>15432989</td><td>&quot;Vivaldi The Four Seasons Winte…</td><td>15432989</td><td>87464</td><td>&quot;Albrecht Mayer&quot;</td><td>14145191</td><td>&quot;Vivaldi Baroque Legends&quot;</td><td>23</td><td>&quot;Classical&quot;</td></tr><tr><td>9883457</td><td>&quot;World Wide Sunrise&quot;</td><td>9883457</td><td>555749</td><td>&quot;Damian Wasse&quot;</td><td>2711145</td><td>&quot;Dreamtrance Party&quot;</td><td>86</td><td>&quot;Trance&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌──────────┬────────────┬────────────┬───────────┬───┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ track_id ┆ track_clea ┆ track_grou ┆ artist_id ┆ … ┆ album_id ┆ album_cle ┆ genre_id ┆ genre_cle │\n",
       "│ ---      ┆ n          ┆ p_id       ┆ ---       ┆   ┆ ---      ┆ an        ┆ ---      ┆ an        │\n",
       "│ i64      ┆ ---        ┆ ---        ┆ i64       ┆   ┆ i64      ┆ ---       ┆ i64      ┆ ---       │\n",
       "│          ┆ str        ┆ i64        ┆           ┆   ┆          ┆ str       ┆          ┆ str       │\n",
       "╞══════════╪════════════╪════════════╪═══════════╪═══╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 15421156 ┆ Saint      ┆ 15421156   ┆ 378040    ┆ … ┆ 10515261 ┆ Classical ┆ 69       ┆ Classical │\n",
       "│          ┆ Saens Le   ┆            ┆           ┆   ┆          ┆ 100       ┆          ┆ music     │\n",
       "│          ┆ Carnaval   ┆            ┆           ┆   ┆          ┆           ┆          ┆           │\n",
       "│          ┆ Des An…    ┆            ┆           ┆   ┆          ┆           ┆          ┆           │\n",
       "│ 15431752 ┆ Verdi      ┆ 15431752   ┆ 83201     ┆ … ┆ 31947    ┆ Verdi     ┆ 117      ┆ Classical │\n",
       "│          ┆ Rigoletto  ┆            ┆           ┆   ┆          ┆ Rigoletto ┆          ┆ masterpie │\n",
       "│          ┆ Act Ii     ┆            ┆           ┆   ┆          ┆           ┆          ┆ ces       │\n",
       "│          ┆ Cortigi…   ┆            ┆           ┆   ┆          ┆           ┆          ┆           │\n",
       "│ 15432989 ┆ Vivaldi    ┆ 15432989   ┆ 87464     ┆ … ┆ 13427069 ┆ In The    ┆ 69       ┆ Classical │\n",
       "│          ┆ The Four   ┆            ┆           ┆   ┆          ┆ Mood For  ┆          ┆ music     │\n",
       "│          ┆ Seasons    ┆            ┆           ┆   ┆          ┆ Vivaldi   ┆          ┆           │\n",
       "│          ┆ Winte…     ┆            ┆           ┆   ┆          ┆           ┆          ┆           │\n",
       "│ 15432989 ┆ Vivaldi    ┆ 15432989   ┆ 87464     ┆ … ┆ 14145191 ┆ Vivaldi   ┆ 23       ┆ Classical │\n",
       "│          ┆ The Four   ┆            ┆           ┆   ┆          ┆ Baroque   ┆          ┆           │\n",
       "│          ┆ Seasons    ┆            ┆           ┆   ┆          ┆ Legends   ┆          ┆           │\n",
       "│          ┆ Winte…     ┆            ┆           ┆   ┆          ┆           ┆          ┆           │\n",
       "│ 9883457  ┆ World Wide ┆ 9883457    ┆ 555749    ┆ … ┆ 2711145  ┆ Dreamtran ┆ 86       ┆ Trance    │\n",
       "│          ┆ Sunrise    ┆            ┆           ┆   ┆          ┆ ce Party  ┆          ┆           │\n",
       "└──────────┴────────────┴────────────┴───────────┴───┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90bfd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp=items.filter(pl.col('track_group_id') == 178480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90263d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.select(pl.col('artist_clean').unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c084642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>track_id</th><th>track_clean</th><th>track_group_id</th><th>artist_id</th><th>artist_clean</th><th>album_id</th><th>album_clean</th><th>genre_id</th><th>genre_clean</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>19237423</td><td>&quot;In The End&quot;</td><td>19237423</td><td>3127658</td><td>&quot;Tomx&quot;</td><td>3545921</td><td>&quot;Electronic Dance Music Miami T…</td><td>101</td><td>&quot;House&quot;</td></tr><tr><td>178480</td><td>&quot;In The End&quot;</td><td>178480</td><td>36800</td><td>&quot;Linkin Park&quot;</td><td>16252</td><td>&quot;In The End&quot;</td><td>41</td><td>&quot;Numetal&quot;</td></tr><tr><td>470224</td><td>&quot;In The End&quot;</td><td>470224</td><td>131549</td><td>&quot;Charlotte Gainsbourg&quot;</td><td>49596</td><td>&quot;Irm&quot;</td><td>11</td><td>&quot;Pop&quot;</td></tr><tr><td>1621152</td><td>&quot;In The End&quot;</td><td>1621152</td><td>352469</td><td>&quot;Stream Of Passion&quot;</td><td>162492</td><td>&quot;The Flame Within&quot;</td><td>43</td><td>&quot;Epicmetal&quot;</td></tr><tr><td>1621152</td><td>&quot;In The End&quot;</td><td>1621152</td><td>352469</td><td>&quot;Stream Of Passion&quot;</td><td>7210462</td><td>&quot;Flame Within&quot;</td><td>43</td><td>&quot;Epicmetal&quot;</td></tr><tr><td>5500863</td><td>&quot;In The End&quot;</td><td>5500863</td><td>452194</td><td>&quot;Black Veil Brides&quot;</td><td>16525363</td><td>&quot;Punk Music&quot;</td><td>47</td><td>&quot;Metal&quot;</td></tr><tr><td>5500863</td><td>&quot;In The End&quot;</td><td>5500863</td><td>452194</td><td>&quot;Black Veil Brides&quot;</td><td>20342138</td><td>&quot;Emo Pop Punk&quot;</td><td>47</td><td>&quot;Metal&quot;</td></tr><tr><td>35951269</td><td>&quot;In The End&quot;</td><td>35951269</td><td>564433</td><td>&quot;Barbara Mendes&quot;</td><td>4506355</td><td>&quot;Superstar&quot;</td><td>138</td><td>&quot;Latinfolk&quot;</td></tr><tr><td>35951269</td><td>&quot;In The End&quot;</td><td>35951269</td><td>564433</td><td>&quot;Barbara Mendes&quot;</td><td>4506355</td><td>&quot;Superstar&quot;</td><td>21</td><td>&quot;Folk&quot;</td></tr><tr><td>39075991</td><td>&quot;In The End&quot;</td><td>39075991</td><td>1087991</td><td>&quot;Akacia&quot;</td><td>5027979</td><td>&quot;In The End&quot;</td><td>68</td><td>&quot;Electronics&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 9)\n",
       "┌──────────┬────────────┬────────────┬───────────┬───┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ track_id ┆ track_clea ┆ track_grou ┆ artist_id ┆ … ┆ album_id ┆ album_cle ┆ genre_id ┆ genre_cle │\n",
       "│ ---      ┆ n          ┆ p_id       ┆ ---       ┆   ┆ ---      ┆ an        ┆ ---      ┆ an        │\n",
       "│ i64      ┆ ---        ┆ ---        ┆ i64       ┆   ┆ i64      ┆ ---       ┆ i64      ┆ ---       │\n",
       "│          ┆ str        ┆ i64        ┆           ┆   ┆          ┆ str       ┆          ┆ str       │\n",
       "╞══════════╪════════════╪════════════╪═══════════╪═══╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 19237423 ┆ In The End ┆ 19237423   ┆ 3127658   ┆ … ┆ 3545921  ┆ Electroni ┆ 101      ┆ House     │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ c Dance   ┆          ┆           │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Music     ┆          ┆           │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Miami T…  ┆          ┆           │\n",
       "│ 178480   ┆ In The End ┆ 178480     ┆ 36800     ┆ … ┆ 16252    ┆ In The    ┆ 41       ┆ Numetal   │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ End       ┆          ┆           │\n",
       "│ 470224   ┆ In The End ┆ 470224     ┆ 131549    ┆ … ┆ 49596    ┆ Irm       ┆ 11       ┆ Pop       │\n",
       "│ 1621152  ┆ In The End ┆ 1621152    ┆ 352469    ┆ … ┆ 162492   ┆ The Flame ┆ 43       ┆ Epicmetal │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Within    ┆          ┆           │\n",
       "│ 1621152  ┆ In The End ┆ 1621152    ┆ 352469    ┆ … ┆ 7210462  ┆ Flame     ┆ 43       ┆ Epicmetal │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Within    ┆          ┆           │\n",
       "│ 5500863  ┆ In The End ┆ 5500863    ┆ 452194    ┆ … ┆ 16525363 ┆ Punk      ┆ 47       ┆ Metal     │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Music     ┆          ┆           │\n",
       "│ 5500863  ┆ In The End ┆ 5500863    ┆ 452194    ┆ … ┆ 20342138 ┆ Emo Pop   ┆ 47       ┆ Metal     │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ Punk      ┆          ┆           │\n",
       "│ 35951269 ┆ In The End ┆ 35951269   ┆ 564433    ┆ … ┆ 4506355  ┆ Superstar ┆ 138      ┆ Latinfolk │\n",
       "│ 35951269 ┆ In The End ┆ 35951269   ┆ 564433    ┆ … ┆ 4506355  ┆ Superstar ┆ 21       ┆ Folk      │\n",
       "│ 39075991 ┆ In The End ┆ 39075991   ┆ 1087991   ┆ … ┆ 5027979  ┆ In The    ┆ 68       ┆ Electroni │\n",
       "│          ┆            ┆            ┆           ┆   ┆          ┆ End       ┆          ┆ cs        │\n",
       "└──────────┴────────────┴────────────┴───────────┴───┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.filter(pl.col('track_clean').str.to_lowercase() == 'in the end'.lower()).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d2a480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>track_id</th><th>listen_count</th><th>last_listen</th></tr><tr><td>i32</td><td>i64</td><td>u32</td><td>date</td></tr></thead><tbody><tr><td>668</td><td>74566</td><td>1</td><td>2022-09-04</td></tr><tr><td>673</td><td>78780542</td><td>1</td><td>2022-11-19</td></tr><tr><td>677</td><td>676087</td><td>1</td><td>2022-08-26</td></tr><tr><td>677</td><td>693906</td><td>1</td><td>2022-08-27</td></tr><tr><td>677</td><td>4777100</td><td>1</td><td>2022-09-04</td></tr><tr><td>677</td><td>20412797</td><td>1</td><td>2022-09-18</td></tr><tr><td>677</td><td>33315053</td><td>1</td><td>2022-10-07</td></tr><tr><td>677</td><td>35419758</td><td>1</td><td>2022-11-10</td></tr><tr><td>677</td><td>15397472</td><td>1</td><td>2022-11-12</td></tr><tr><td>678</td><td>61886459</td><td>1</td><td>2022-09-03</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌─────────┬──────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ track_id ┆ listen_count ┆ last_listen │\n",
       "│ ---     ┆ ---      ┆ ---          ┆ ---         │\n",
       "│ i32     ┆ i64      ┆ u32          ┆ date        │\n",
       "╞═════════╪══════════╪══════════════╪═════════════╡\n",
       "│ 668     ┆ 74566    ┆ 1            ┆ 2022-09-04  │\n",
       "│ 673     ┆ 78780542 ┆ 1            ┆ 2022-11-19  │\n",
       "│ 677     ┆ 676087   ┆ 1            ┆ 2022-08-26  │\n",
       "│ 677     ┆ 693906   ┆ 1            ┆ 2022-08-27  │\n",
       "│ 677     ┆ 4777100  ┆ 1            ┆ 2022-09-04  │\n",
       "│ 677     ┆ 20412797 ┆ 1            ┆ 2022-09-18  │\n",
       "│ 677     ┆ 33315053 ┆ 1            ┆ 2022-10-07  │\n",
       "│ 677     ┆ 35419758 ┆ 1            ┆ 2022-11-10  │\n",
       "│ 677     ┆ 15397472 ┆ 1            ┆ 2022-11-12  │\n",
       "│ 678     ┆ 61886459 ┆ 1            ┆ 2022-09-03  │\n",
       "└─────────┴──────────┴──────────────┴─────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(events.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c7cd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------- Clean up interactions dataframe (canonical track ids) ---------- #\u001b[39;00m\n\u001b[1;32m      3\u001b[0m interactions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 4\u001b[0m     \u001b[43minteractions\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Drop observations with track_seq bigger than 95% percentile\u001b[39;00m\n\u001b[1;32m     10\u001b[0m track_seq_upper_threshold \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     interactions\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m.\u001b[39mselect(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_seq\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.95\u001b[39m))\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interactions' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------- Clean up interactions dataframe (canonical track ids) ---------- #\n",
    "\n",
    "interactions = (\n",
    "    interactions\n",
    "        .with_columns(pl.col('started_at').dt.date().alias('started_at'))\n",
    "        .drop('__index_level_0__')\n",
    ")\n",
    "\n",
    "# Drop observations with track_seq bigger than 95% percentile\n",
    "track_seq_upper_threshold = (\n",
    "    interactions\n",
    "        .select(pl.col('track_seq').quantile(0.95))\n",
    "        .item()\n",
    ")\n",
    "\n",
    "print('Upper threshold track_seq:', track_seq_upper_threshold)\n",
    "\n",
    "interactions = (\n",
    "    interactions\n",
    "        .filter(pl.col('track_seq') <= track_seq_upper_threshold)\n",
    "        .join(track_id_map, on='track_id', how='left')\n",
    "        .with_columns([\n",
    "            pl.col('track_id_canonical').alias('track_id')\n",
    "        ])\n",
    "        .drop_nulls(['track_id'])\n",
    "        .select(['user_id', 'track_id', 'track_seq', 'started_at'])\n",
    ")\n",
    "\n",
    "data_summary(interactions, 'interactions (canonical tracks)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b573a-9e2d-4808-95db-60cfb8bbdb73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb65c5c-3c2a-412a-ab7e-30ec9da54fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc3296b-eba6-4333-a78d-b9304aa87e3d",
   "metadata": {},
   "source": [
    "# === ЭТАП 2 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e73960-fd38-4e15-8db0-9a25c35dfd25",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e823e-8e0f-4a76-a02e-8d1ba8bf0f8a",
   "metadata": {},
   "source": [
    "Распределение количества прослушанных треков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765519a-18dd-4d30-9e29-cc2d84cacd79",
   "metadata": {},
   "source": [
    "Наиболее популярные треки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_by_listen_number = (\n",
    "    interactions\n",
    "        .group_by('track_id')\n",
    "        .agg(pl.count('track_seq').alias('listen_count'))\n",
    "        .join(tracks_catalog_clean.select(['track_id', 'track_clean']), on='track_id', how='left')\n",
    "        .sort('listen_count', descending=True)\n",
    ")\n",
    "\n",
    "top_tracks = tracks_by_listen_number.head(10)\n",
    "display(top_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c32a5a-d3be-4f96-8dd9-f7860951020c",
   "metadata": {},
   "source": [
    "Наиболее популярные жанры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 genres by listening number\n",
    "genres_by_listen_count = (\n",
    "    interactions\n",
    "        .group_by('track_id')\n",
    "        .agg(pl.len().alias('track_listen_count'))\n",
    "        .join(\n",
    "            items.select(['track_id', 'genre_clean']).unique(['track_id', 'genre_clean']),\n",
    "            on='track_id',\n",
    "            how='left'\n",
    "        )\n",
    "        .group_by('genre_clean')\n",
    "        .agg(pl.sum('track_listen_count').alias('listen_count'))\n",
    "        .sort('listen_count', descending=True)\n",
    ")\n",
    "\n",
    "top_5_genres = genres_by_listen_count.head(5)\n",
    "display(top_5_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15254e66-e80e-473b-ba24-abebea5ccac7",
   "metadata": {},
   "source": [
    "Треки, которые никто не прослушал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54020fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks that haven't been listened to by anybody\n",
    "unlistened_tracks = (\n",
    "    items\n",
    "        .select(['track_id', 'track_clean', 'artist_clean', 'album_clean', 'genre_clean'])\n",
    "        .unique('track_id')\n",
    "        .join(\n",
    "            interactions.select('track_id').unique(),\n",
    "            on='track_id',\n",
    "            how='anti'\n",
    "        )\n",
    ")\n",
    "\n",
    "print(f'Number of unlistened tracks: {unlistened_tracks.height}')\n",
    "print(unlistened_tracks.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d850a07-ef1e-462f-891a-1cf89f2e24ef",
   "metadata": {},
   "source": [
    "# Преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcf8d2-1192-4df5-b20b-fbb84689f57a",
   "metadata": {},
   "source": [
    "Преобразуем данные в формат, более пригодный для дальнейшего использования в расчётах рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up events dataset by aggregating interactions: count listens per user-track pair\n",
    "# Polars lazy mode with streaming used in order not to crash kernel by dealing with the whole dataset at once\n",
    "events = (\n",
    "    interactions.lazy()\n",
    "        .group_by(['user_id', 'track_id'])\n",
    "        .agg([\n",
    "            pl.len().alias('listen_count'),\n",
    "            pl.max('started_at').alias('last_listen')\n",
    "        ])\n",
    "        .sort(['user_id', 'listen_count'], descending=[False, True])\n",
    "        .collect(engine='streaming')\n",
    ")\n",
    "print(f\"Aggregated interactions: {events.shape}\")\n",
    "print(events.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68241760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders for user_id and track_id\n",
    "# ALS requires consecutive integer indices starting from 0\n",
    "user_encoder = LabelEncoder()\n",
    "track_encoder = LabelEncoder()\n",
    "# Fit encoders\n",
    "user_encoder.fit(events['user_id'].to_numpy())\n",
    "track_encoder.fit(events['track_id'].to_numpy())\n",
    "# Transform to encoded indices\n",
    "events_data = (\n",
    "    events\n",
    "        .with_columns([\n",
    "            pl.Series('user_idx', user_encoder.transform(events['user_id'].to_numpy())),\n",
    "            pl.Series('track_idx', track_encoder.transform(events['track_id'].to_numpy()))\n",
    "        ])\n",
    ")\n",
    "print(f\"Encoded data shape: {events_data.shape}\")\n",
    "print(f\"Unique users: {events_data['user_idx'].n_unique()}\")\n",
    "print(f\"Unique tracks: {events_data['track_idx'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scipy sparse matrix (COO format)\n",
    "# Using log-transformed listen counts as implicit feedback weights\n",
    "user_track_sparse = scipy.sparse.coo_matrix(\n",
    "    (\n",
    "        np.log1p(events_data['listen_count'].to_numpy()),  # values (log-scaled)\n",
    "        (\n",
    "            events_data['user_idx'].to_numpy(),  # row indices\n",
    "            events_data['track_idx'].to_numpy()   # col indices\n",
    "        )\n",
    "    ),\n",
    "    shape=(\n",
    "        events_data['user_idx'].max() + 1,\n",
    "        events_data['track_idx'].max() + 1\n",
    "    )\n",
    ")\n",
    "# Convert to CSR format for efficient row operations\n",
    "user_track_sparse = user_track_sparse.tocsr()\n",
    "print(f\"Sparse matrix shape: {user_track_sparse.shape}\")\n",
    "print(f\"Sparsity: {1 - user_track_sparse.nnz / (user_track_sparse.shape[0] * user_track_sparse.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6272bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoders for decoding predictions back to original ids\n",
    "encoder_mappings = {\n",
    "    'user_encoder': user_encoder,\n",
    "    'track_encoder': track_encoder,\n",
    "    'user_id_to_idx': dict(zip(events['user_id'].unique(), \n",
    "                                user_encoder.transform(events['user_id'].unique()))),\n",
    "    'track_id_to_idx': dict(zip(events['track_id'].unique(),\n",
    "                                 track_encoder.transform(events['track_id'].unique())))\n",
    "}\n",
    "\n",
    "# Save encoder locally\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "with open(os.path.join(encoder_dir, 'encoder_mappings.joblib'), 'wb') as f:\n",
    "    joblib.dump(encoder_mappings, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd8a7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1be653-eed8-4fa0-a9c5-3811b080d71d",
   "metadata": {},
   "source": [
    "# Сохранение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1314a-3af3-4faf-a0b1-3045173d1500",
   "metadata": {},
   "source": [
    "Сохраним данные в двух файлах в персональном S3-бакете по пути `recsys/data/`:\n",
    "- `items.parquet` — все данные о музыкальных треках,\n",
    "- `events.parquet` — все данные о взаимодействиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77af17-7705-4fae-9aad-db579b39faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets locally\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "items.write_parquet(os.path.join(preprocessed_dir, 'items.parquet'))\n",
    "events.write_parquet(os.path.join(preprocessed_dir, 'events.parquet'))\n",
    "print(f'Files saved locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48098d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save data to S3 bucket\n",
    "#\n",
    "## Initialize S3 client\n",
    "#s3_client = boto3.client(\n",
    "#    's3',\n",
    "#    region_name=s3_region,\n",
    "#    aws_access_key_id=aws_access_key_id,\n",
    "#    aws_secret_access_key=aws_secret_access_key\n",
    "#)\n",
    "#\n",
    "#def upload_to_s3(local_path, s3_key):\n",
    "#    '''\n",
    "#        Upload a file to S3 bucket\n",
    "#    '''\n",
    "#    try:\n",
    "#        s3_client.upload_file(local_path, s3_bucket, s3_key)\n",
    "#        print(f'Uploaded {local_path} to s3://{s3_bucket}/{s3_key}')\n",
    "#    except ClientError as e:\n",
    "#        print(f'Error uploading {local_path}: {e}')\n",
    "#        raise\n",
    "#\n",
    "## Upload to S3\n",
    "#upload_to_s3(\n",
    "#    os.path.join(preprocessed_dir, 'items.parquet'),\n",
    "#    f'{s3_prefix}items.parquet'\n",
    "#)\n",
    "#upload_to_s3(\n",
    "#    os.path.join(preprocessed_dir, 'events.parquet'),\n",
    "#    f'{s3_prefix}events.parquet'\n",
    "#)\n",
    "#\n",
    "#print(f'All files uploaded to S3 bucket: {s3_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecbbed-c560-44d9-9c14-86c7dc76f399",
   "metadata": {},
   "source": [
    "# Очистка памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5358ede-ba6e-4c4f-bd73-5b9344f0ba79",
   "metadata": {},
   "source": [
    "Здесь, может понадобится очистка памяти для высвобождения ресурсов для выполнения кода ниже. \n",
    "\n",
    "Приведите соответствующие код, комментарии, например:\n",
    "- код для удаление более ненужных переменных,\n",
    "- комментарий, что следует перезапустить kernel, выполнить такие-то начальные секции и продолжить с этапа 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fe920-e12e-4ad8-b04e-56e8091fac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unnecessary variables to free memory\n",
    "\n",
    "# List of objects to delete\n",
    "variables_to_delete = [\n",
    "    'interactions',\n",
    "    'tracks',\n",
    "    'catalog_names',\n",
    "    'tracks_exploded',\n",
    "    'user_track_interactions',\n",
    "    'albums_catalog',\n",
    "    'artists_catalog',\n",
    "    'genres_catalog',\n",
    "    'tracks_catalog',\n",
    "    'album_duplicates',\n",
    "    'artist_duplicates',\n",
    "    'genre_duplicates',\n",
    "    'track_duplicates',\n",
    "    'album_id_map',\n",
    "    'artist_id_map',\n",
    "    'genre_id_map',\n",
    "    'track_id_map',\n",
    "    'albums_dedup',\n",
    "    'artists_dedup',\n",
    "    'genres_dedup',\n",
    "    'tracks_dedup'\n",
    "]\n",
    "\n",
    "# Delete variables\n",
    "for var in variables_to_delete:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "        print(f\"Deleted {var}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print('Memory cleanup complete')\n",
    "print('To fully free memory, restart the kernel:')\n",
    "print('  1. Click \"Kernel\" → \"Restart Kernel...\"')\n",
    "print('  2. Re-run initial cells:')\n",
    "print('     - Cell 3: Imports')\n",
    "print('     - Cell 5: Config')\n",
    "print('  3. Load preprocessed data:')\n",
    "print(\"     items = pl.read_parquet('../data/preprocessed/items.parquet')\")\n",
    "print(\"     events = pl.read_parquet('../data/preprocessed/events.parquet')\")\n",
    "print(\"  4. Continue from Stage 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708503df-ee89-4cf3-8489-093dc478e2a8",
   "metadata": {},
   "source": [
    "# === ЭТАП 3 ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77de22-e10f-4b42-85c1-8fb6f805fe68",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a4241-1ecd-4a3e-bbb3-fc2f6ca94f68",
   "metadata": {},
   "source": [
    "Если необходимо, то загружаем items.parquet, events.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fc8a5-bd2c-40d7-864a-ee75aca6d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#items = pl.read_parquet('../data/preprocessed/items.parquet')\n",
    "#events = pl.read_parquet('../data/preprocessed/events.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694c023-6477-490b-939d-1cfa6f5f1b72",
   "metadata": {},
   "source": [
    "# Разбиение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5f6e0-54e7-4428-8678-eabce505d82c",
   "metadata": {},
   "source": [
    "Разбиваем данные на тренировочную, тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2dfa5-d8a2-47d1-922e-6eefee2c62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Split data chronologically ---------- #\n",
    "\n",
    "# Define split date\n",
    "# Convert date to days since epoch, find quantile, convert back as Polars cannot handle date quantiles\n",
    "date_threshold = (\n",
    "    events\n",
    "        .select(\n",
    "            pl.col('last_listen')\n",
    "              .cast(pl.Date)\n",
    "              .to_physical()\n",
    "              .quantile(0.8)\n",
    "              .cast(pl.Int32)\n",
    "        )\n",
    "        .item()\n",
    ")\n",
    "\n",
    "# Convert back to date\n",
    "date_threshold = pl.Series([date_threshold]).cast(pl.Date).item()\n",
    "print(f'Split date: {date_threshold}')\n",
    "\n",
    "# Split based on time\n",
    "train_events = events.filter(pl.col('last_listen') <= date_threshold)\n",
    "test_events = events.filter(pl.col('last_listen') > date_threshold)\n",
    "\n",
    "print(f'Train set: {train_events.shape[0]:,}')\n",
    "print(f'Test set: {test_events.shape[0]:,}')\n",
    "print(f'Split ratio: {train_events.shape[0]/events.shape[0]:.1%} / {test_events.shape[0]/events.shape[0]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131c7e6-8852-4556-b510-51f7253cc299",
   "metadata": {},
   "source": [
    "# Топ популярных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70d43a-88cc-4719-b291-feaed7136f30",
   "metadata": {},
   "source": [
    "Рассчитаем рекомендации как топ популярных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45e200-b7d6-4f56-9077-aad431689b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find popularity score from training data and get top 100 tracks\n",
    "\n",
    "def get_popular_tracks(train_events, tracks_catalog, items=None, top_n=100, min_users=10, max_avg_listens=50):\n",
    "    ''' \n",
    "        Get most popular tracks with protection against data corruption.\n",
    "        \n",
    "        Parameters:\n",
    "        - train_events: aggregated user-track interactions\n",
    "        - tracks_catalog: deduplicated track catalog (tracks_catalog_clean)\n",
    "        - items: optional DataFrame for genre/artist info\n",
    "        - top_n: number of top tracks to return\n",
    "        - min_users: minimum unique users required\n",
    "        - max_avg_listens: maximum average listens per user (anti-bot protection)\n",
    "        \n",
    "        Anti-bot protection applied as filters of:\n",
    "        - minimum user_count,\n",
    "        - maximum average listens per user.\n",
    "        \n",
    "        Popularity score is calculated as multiplicative combination of \n",
    "        log(total_listens) * log(user_count), \n",
    "        so low user_count works as penalty and drastically reduces score.\n",
    "\n",
    "        Return is the top-N tracks with highest popularity score, each track_id \n",
    "        combined with the most common genre and artists\n",
    "        so that track in the top is original and not a remix or cover.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with track_id, popularity_score, and optional genre, artist, album. \n",
    "    '''\n",
    "    \n",
    "    popular_tracks = (\n",
    "        train_events\n",
    "            .group_by('track_id')\n",
    "            .agg([\n",
    "                pl.sum('listen_count').alias('total_listens'),\n",
    "                pl.len().alias('user_count')  \n",
    "            ])\n",
    "            # Calculate average listens per user\n",
    "            .with_columns([\n",
    "                (pl.col('total_listens') / pl.col('user_count')).alias('avg_per_user')\n",
    "            ])\n",
    "            # Filter suspicious tracks\n",
    "            .filter(\n",
    "                (pl.col('user_count') >= min_users) &  # Minimum user diversity\n",
    "                (pl.col('avg_per_user') <= max_avg_listens)  # Anti-bot filter\n",
    "            )\n",
    "            # Multiplicative popularity score (both must be high in order to get a high popularity score)\n",
    "            .with_columns([\n",
    "                (pl.col('total_listens').log1p() * \n",
    "                 pl.col('user_count').log1p()).alias('popularity_score')\n",
    "            ])\n",
    "            .sort('popularity_score', descending=True)\n",
    "            .head(top_n)\n",
    "    )\n",
    "    \n",
    "    # Join with deduplicated track catalog to get unique track names\n",
    "    popular_tracks_with_info = (\n",
    "        popular_tracks\n",
    "            .join(\n",
    "                tracks_catalog.select(['track_id', 'track_clean']),\n",
    "                on='track_id',\n",
    "                how='left'\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    # Add most common genre and artist for each track\n",
    "    if items is not None:\n",
    "        track_meta = (\n",
    "            items\n",
    "                .group_by('track_id')\n",
    "                .agg([\n",
    "                    pl.col('genre_clean').mode().first().alias('genre_clean'),\n",
    "                    pl.col('artist_clean').mode().first().alias('artist_clean'),\n",
    "                    pl.col('album_clean').mode().first().alias('album_clean')\n",
    "                ])\n",
    "        )\n",
    "        popular_tracks_with_info = popular_tracks_with_info.join(\n",
    "            track_meta,\n",
    "            on='track_id',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return popular_tracks_with_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 100 popular tracks\n",
    "top_popular = get_popular_tracks(\n",
    "    train_events=train_events, \n",
    "    tracks_catalog=tracks_catalog_clean,\n",
    "    items=items,\n",
    "    top_n=100, \n",
    "    min_users=10, \n",
    "    max_avg_listens=50\n",
    ")\n",
    "\n",
    "print(f'Top 10 Popular Tracks:')\n",
    "display(top_popular.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad231f2-6158-421a-b7fa-01d8bc3ed572",
   "metadata": {},
   "source": [
    "# Персональные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86159460-cd9d-4b63-8248-604ea3c9aebf",
   "metadata": {},
   "source": [
    "Рассчитаем персональные рекомендации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cdb58-3a8c-45ad-8e5f-7f950314aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Prepare Data for ALS Model\n",
    "\n",
    "# Encode user and track IDs for train data\n",
    "user_encoder = LabelEncoder()\n",
    "track_encoder = LabelEncoder()\n",
    "\n",
    "# Fit encoders on training data only\n",
    "user_encoder.fit(train_events['user_id'].to_numpy())\n",
    "track_encoder.fit(train_events['track_id'].to_numpy())\n",
    "\n",
    "# Transform training events\n",
    "train_events_encoded = train_events.with_columns([\n",
    "    pl.Series('user_idx', user_encoder.transform(train_events['user_id'].to_numpy())),\n",
    "    pl.Series('track_idx', track_encoder.transform(train_events['track_id'].to_numpy()))\n",
    "])\n",
    "\n",
    "print(f\"Encoded training data shape: {train_events_encoded.shape}\")\n",
    "print(f\"Unique users: {train_events_encoded['user_idx'].n_unique()}\")\n",
    "print(f\"Unique tracks: {train_events_encoded['track_idx'].n_unique()}\")\n",
    "\n",
    "# Create sparse user-track matrix (CSR format for efficiency)\n",
    "user_track_sparse = scipy.sparse.coo_matrix(\n",
    "    (\n",
    "        np.log1p(train_events_encoded['listen_count'].to_numpy()),  # log-scaled weights\n",
    "        (\n",
    "            train_events_encoded['user_idx'].to_numpy(),  # row indices\n",
    "            train_events_encoded['track_idx'].to_numpy()   # col indices\n",
    "        )\n",
    "    ),\n",
    "    shape=(\n",
    "        train_events_encoded['user_idx'].max() + 1,\n",
    "        train_events_encoded['track_idx'].max() + 1\n",
    "    )\n",
    ").tocsr()\n",
    "\n",
    "print(f\"\\nSparse matrix shape: {user_track_sparse.shape}\")\n",
    "print(f\"Sparsity: {1 - user_track_sparse.nnz / (user_track_sparse.shape[0] * user_track_sparse.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Train ALS Model\n",
    "\n",
    "# Initialize ALS model\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=64,              # Number of latent factors\n",
    "    regularization=0.01,     # L2 regularization\n",
    "    iterations=15,           # Number of training iterations\n",
    "    calculate_training_loss=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train the model (items x users matrix for implicit library)\n",
    "print(\"Training ALS model...\")\n",
    "als_model.fit(user_track_sparse.T.tocsr(), show_progress=True)\n",
    "\n",
    "print(\"\\n✓ ALS model trained successfully\")\n",
    "print(f\"Factors: {als_model.factors}\")\n",
    "print(f\"User factors shape: {als_model.user_factors.shape}\")\n",
    "print(f\"Item factors shape: {als_model.item_factors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Generate Personal Recommendations\n",
    "\n",
    "def get_personal_recommendations(\n",
    "    user_id, \n",
    "    als_model, \n",
    "    user_encoder, \n",
    "    track_encoder, \n",
    "    user_track_sparse,\n",
    "    items,\n",
    "    n_recommendations=10,\n",
    "    filter_already_listened=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Get personalized recommendations for a user using ALS model.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: original user ID\n",
    "    - als_model: trained ALS model\n",
    "    - user_encoder: fitted LabelEncoder for users\n",
    "    - track_encoder: fitted LabelEncoder for tracks\n",
    "    - user_track_sparse: sparse user-track matrix\n",
    "    - items: items DataFrame with track information\n",
    "    - n_recommendations: number of recommendations to return\n",
    "    - filter_already_listened: whether to filter out already listened tracks\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with recommended tracks and scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Encode user_id\n",
    "        user_idx = user_encoder.transform([user_id])[0]\n",
    "    except ValueError:\n",
    "        # User not in training set (cold start)\n",
    "        print(f\"User {user_id} not found in training data. Returning popular tracks.\")\n",
    "        return get_popular_tracks(train_events, items, top_n=n_recommendations)\n",
    "    \n",
    "    # Get recommendations from ALS\n",
    "    track_ids, scores = als_model.recommend(\n",
    "        user_idx,\n",
    "        user_track_sparse[user_idx],\n",
    "        N=n_recommendations + 100,  # Get more to filter out listened tracks\n",
    "        filter_already_liked_items=filter_already_listened\n",
    "    )\n",
    "    \n",
    "    # Decode track indices to original track IDs\n",
    "    recommended_track_ids = track_encoder.inverse_transform(track_ids)\n",
    "    \n",
    "    # Create DataFrame with recommendations\n",
    "    recommendations = pl.DataFrame({\n",
    "        'track_id': recommended_track_ids[:n_recommendations],\n",
    "        'score': scores[:n_recommendations]\n",
    "    })\n",
    "    \n",
    "    # Join with items to get track details\n",
    "    recommendations_with_info = recommendations.join(\n",
    "        items.select(['track_id', 'albums', 'artists', 'genres']),\n",
    "        on='track_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return recommendations_with_info\n",
    "\n",
    "\n",
    "# Test recommendations for a few users\n",
    "test_user_ids = train_events['user_id'].unique().head(5).to_list()\n",
    "\n",
    "for user_id in test_user_ids:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Recommendations for User ID: {user_id}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    recs = get_personal_recommendations(\n",
    "        user_id=user_id,\n",
    "        als_model=als_model,\n",
    "        user_encoder=user_encoder,\n",
    "        track_encoder=track_encoder,\n",
    "        user_track_sparse=user_track_sparse,\n",
    "        items=items,\n",
    "        n_recommendations=10\n",
    "    )\n",
    "    \n",
    "    print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Generate Recommendations for All Users\n",
    "\n",
    "def generate_all_recommendations(\n",
    "    user_ids,\n",
    "    als_model,\n",
    "    user_encoder,\n",
    "    track_encoder,\n",
    "    user_track_sparse,\n",
    "    items,\n",
    "    top_popular,\n",
    "    n_recommendations=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users (with fallback to popular for cold start).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with user_id, track_id, score, rank\n",
    "    \"\"\"\n",
    "    all_recommendations = []\n",
    "    \n",
    "    for i, user_id in enumerate(user_ids):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing user {i}/{len(user_ids)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to get personalized recommendations\n",
    "            user_idx = user_encoder.transform([user_id])[0]\n",
    "            track_ids, scores = als_model.recommend(\n",
    "                user_idx,\n",
    "                user_track_sparse[user_idx],\n",
    "                N=n_recommendations,\n",
    "                filter_already_liked_items=True\n",
    "            )\n",
    "            recommended_track_ids = track_encoder.inverse_transform(track_ids)\n",
    "            \n",
    "        except (ValueError, IndexError):\n",
    "            # Cold start user - use popular tracks\n",
    "            recommended_track_ids = top_popular['track_id'].head(n_recommendations).to_numpy()\n",
    "            scores = top_popular['popularity_score'].head(n_recommendations).to_numpy()\n",
    "        \n",
    "        # Create recommendation records\n",
    "        for rank, (track_id, score) in enumerate(zip(recommended_track_ids, scores), 1):\n",
    "            all_recommendations.append({\n",
    "                'user_id': user_id,\n",
    "                'track_id': track_id,\n",
    "                'score': score,\n",
    "                'rank': rank\n",
    "            })\n",
    "    \n",
    "    return pl.DataFrame(all_recommendations)\n",
    "\n",
    "\n",
    "# Generate recommendations for all test users\n",
    "print(\"Generating recommendations for test set users...\")\n",
    "test_user_ids = test_events['user_id'].unique().sort().to_list()\n",
    "\n",
    "test_recommendations = generate_all_recommendations(\n",
    "    user_ids=test_user_ids,\n",
    "    als_model=als_model,\n",
    "    user_encoder=user_encoder,\n",
    "    track_encoder=track_encoder,\n",
    "    user_track_sparse=user_track_sparse,\n",
    "    items=items,\n",
    "    top_popular=top_popular,\n",
    "    n_recommendations=10\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(test_recommendations):,} recommendations\")\n",
    "print(f\"For {test_recommendations['user_id'].n_unique():,} users\")\n",
    "print(test_recommendations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e18c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Save ALS Model and Encoders\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Create encoder directory\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "\n",
    "# Save encoders and model\n",
    "joblib.dump(user_encoder, os.path.join(encoder_dir, 'user_encoder.pkl'))\n",
    "joblib.dump(track_encoder, os.path.join(encoder_dir, 'track_encoder.pkl'))\n",
    "joblib.dump(als_model, os.path.join(encoder_dir, 'als_model.pkl'))\n",
    "\n",
    "# Save popular tracks for cold start\n",
    "top_popular.write_parquet(os.path.join(preprocessed_dir, 'popular_tracks.parquet'))\n",
    "\n",
    "# Save recommendations\n",
    "test_recommendations.write_parquet(os.path.join(preprocessed_dir, 'test_recommendations.parquet'))\n",
    "\n",
    "print(\"✓ Saved:\")\n",
    "print(f\"  - user_encoder.pkl\")\n",
    "print(f\"  - track_encoder.pkl\")\n",
    "print(f\"  - als_model.pkl\")\n",
    "print(f\"  - popular_tracks.parquet\")\n",
    "print(f\"  - test_recommendations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09dc7e-7c91-4355-860a-b9cfb9f33f15",
   "metadata": {},
   "source": [
    "# Похожие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb683-b440-40a8-9975-894156a53872",
   "metadata": {},
   "source": [
    "Рассчитаем похожие, они позже пригодятся для онлайн-рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d07ee-4b12-4ce5-aa85-e45cb7a7a4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce370904-4c49-4152-8706-416074ea9b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0087a3e7-ca9f-42cd-944c-944222c1baef",
   "metadata": {},
   "source": [
    "# Построение признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a32e1-b90b-4eaf-9439-fc8deab9f34b",
   "metadata": {},
   "source": [
    "Построим три признака, можно больше, для ранжирующей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4ae84-406a-44a4-abec-4f80f93e3004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f84c35-f544-4c3d-ad53-9b1d2b684c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47bcf88d-b236-46f0-a6f3-38ddd64895fe",
   "metadata": {},
   "source": [
    "# Ранжирование рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd8223-3418-4493-8c87-1f76286ebda0",
   "metadata": {},
   "source": [
    "Построим ранжирующую модель, чтобы сделать рекомендации более точными. Отранжируем рекомендации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1dd92-32a9-463d-827e-8bb9ee5bbb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe4db4-1ac5-44da-a13c-8e7f9768ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c84071-45b5-4a15-a683-e0ab034a3128",
   "metadata": {},
   "source": [
    "# Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b107fe4-554e-42b1-87d9-c435a52bb77a",
   "metadata": {},
   "source": [
    "Проверим оценку качества трёх типов рекомендаций: \n",
    "\n",
    "- топ популярных,\n",
    "- персональных, полученных при помощи ALS,\n",
    "- итоговых\n",
    "  \n",
    "по четырем метрикам: recall, precision, coverage, novelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6f388-aecb-443e-8647-14014e932d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df411f-14c1-4848-8797-f37afe449cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c8d38c-32b0-46a4-96f0-cd01dac708bc",
   "metadata": {},
   "source": [
    "# === Выводы, метрики ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7d5d8-7d1e-4fdf-a6cd-83e5ce92c684",
   "metadata": {},
   "source": [
    "Основные выводы при работе над расчётом рекомендаций, рассчитанные метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403097d-db36-46d9-8952-613c9bd51b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cfdd5-6f2e-4de6-8666-85804c87d04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
